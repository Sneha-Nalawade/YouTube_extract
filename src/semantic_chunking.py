# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8aLyP5tVu6kqvUHLa9pFJeYX3q1cQ4N

#Step 1: Download Video and Extract Audio
"""

# !pip install pytube
# !pip install moviepy.editor

# from pytube import YouTube
# from moviepy.editor import VideoFileClip

# def download_video_and_extract_audio(video_url):
#     # Download the video
#     yt = YouTube(video_url)
#     video = yt.streams.filter(only_audio=True).first().download()

#     # Extract audio and save it
#     audio_clip = VideoFileClip(video)
#     audio_clip.write_audiofile("output.wav")

#     # Clean-up downloaded video file
#     import os
#     os.remove(video)

# # Example usage
# video_url = "https://www.youtube.com/watch?v=Sby1uJ_NFIY"
# download_video_and_extract_audio(video_url)

# !pip install youtube_dl pydub

# !pip install --upgrade youtube_dl

# import os
# import youtube_dl
# from pydub import AudioSegment

# def download_video_and_extract_audio(video_url):
#     ydl_opts = {
#         'format': 'bestaudio/best',
#         'postprocessors': [{
#             'key': 'FFmpegExtractAudio',
#             'preferredcodec': 'wav',
#             'preferredquality': '192',
#         }],
#         'outtmpl': 'output.%(ext)s',
#         'no-check-certificate': True
#     }

#     with youtube_dl.YoutubeDL(ydl_opts) as ydl:
#         ydl.download([video_url])

#     # Rename the downloaded audio file to output.wav
#     for file in os.listdir('.'):
#         if file.endswith('.wav'):
#             os.rename(file, 'output.wav')

# # Example usage
# video_url = "https://www.youtube.com/watch?v=Sby1uJ_NFIY"
# download_video_and_extract_audio(video_url)

# !pip install pytube

# from pytube import YouTube
# from pydub import AudioSegment

# def download_video_and_extract_audio(video_url):
#     # Download the video
#     yt = YouTube(video_url)
#     stream = yt.streams.filter(only_audio=True).first()
#     stream.download(filename='output')

#     # Convert the downloaded video to WAV format
#     audio = AudioSegment.from_file("output.mp4")
#     audio.export("output.wav", format="wav")

#     # Clean up downloaded video file
#     import os
#     os.remove("output.mp4")

# # Example usage
# video_url = "https://www.youtube.com/watch?v=Sby1uJ_NFIY"
# # download_video_and_extract_audio(video_url)

# from pytube import YouTube
# from pydub import AudioSegment
# import os

# def download_video_and_extract_audio(video_url):
#     # Download the video
#     yt = YouTube(video_url)
#     stream = yt.streams.filter(only_audio=True).first()
#     stream.download(output_path='./', filename='output')

#     # Check if the video downloaded successfully
#     # if not os.path.exists('./output.mp4'):
#     #     raise FileNotFoundError("Downloaded video file not found.")

#     # Convert the downloaded video to WAV format
#     # video_filename = '/content/output'
#     audio_filename = '/content/output'

#     audio = AudioSegment.from_file(video_filename, format="mp4")
#     audio.export(audio_filename, format="wav")

#     # Clean up downloaded video file
#     os.remove(video_filename)

# # Example usage
# video_url = "https://www.youtube.com/watch?v=Sby1uJ_NFIY"
# download_video_and_extract_audio(video_url)

# !ls -l

# import os
# import subprocess

# def download_video(youtube_link, output_dir):
#     command = f"youtube-dl -f bestaudio --output '{output_dir}/%(title)s.%(ext)s' {youtube_link}"
#     subprocess.call(command, shell=True)

# def extract_audio(video_file, output_dir):
#     output_audio_file = os.path.join(output_dir, os.path.splitext(video_file)[0] + ".mp3")
#     command = f"ffmpeg -i {video_file} -vn -acodec mp3 {output_audio_file}"
#     subprocess.call(command, shell=True)
#     return output_audio_file

# # Example usage
# youtube_link = "https://www.youtube.com/watch?v=your_video_id"
# output_dir = "path_to_output_directory"
# download_video(youtube_link, output_dir)
# video_file = os.path.join(output_dir, "your_video_title.mp4")
# audio_file = extract_audio(video_file, output_dir)

# !pip install ffmpeg-python

# from pytube import YouTube
# import ffmpeg

# text = 'https://www.youtube.com/watch?v=Sby1uJ_NFIY'

# yt = YouTube(text)

# # https://github.com/pytube/pytube/issues/301
# stream_url = yt.streams.all()[0].url  # Get the URL of the video stream

# # Probe the audio streams (use it in case you need information like sample rate):
# #probe = ffmpeg.probe(stream_url)
# #audio_streams = next((stream for stream in probe['streams'] if stream['codec_type'] == 'audio'), None)
# #sample_rate = audio_streams['sample_rate']

# # Read audio into memory buffer.
# # Get the audio using stdout pipe of ffmpeg sub-process.
# # The audio is transcoded to PCM codec in WAC container.
# audio, err = (
# #     ffmpeg
#     .input(stream_url, format='wav')
#     .output("pipe:", format='wav', acodec='pcm_s16le')  # Select WAV output format, and pcm_s16le auidio codec. My add ar=sample_rate
#     .run(capture_stdout=True)
# )

# # Write the audio buffer to file for testing
# with open('audio.wav', 'wb') as f:
#     f.write(audio)

import os
!pip install pytube moviepy
from pytube import YouTube
from moviepy.editor import *

# Enter the YouTube video URL
url = 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'

# Download the video
yt = YouTube(url)
stream = yt.streams.get_highest_resolution()
stream.download()

# Extract the audio
video = VideoFileClip('/content/Never Gonna Give You Up.mp4')
audio = video.audio
audio.write_audiofile('/content/outputWAV.mp3')

# Delete the downloaded video file
os.remove('/content/Never Gonna Give You Up.mp4')

!pip install pydub ffmpeg SpeechRecognition

# !ffmpeg -i /content/outputWAV.mp3 -ar 16000 output16.wav
!ffmpeg -i /content/outputWAV.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output16.wav

"""# **Step 2: generating Transcipt from audio file**"""

# import speech_recognition as sr
# from os import path
# from pydub import AudioSegment

# # convert mp3 file to wav
# sound = AudioSegment.from_mp3("/content/Never Gonna Give You Up.mp3")
# sound.export("/content/Never Gonna Give You Up.wav", format="wav")

# r = sr.Recognizer()
# print(r.api_key)
# print(r.language)

# # transcribe audio file
# AUDIO_FILE = "/content/Never Gonna Give You Up.wav"

# # use the audio file as the audio source
# r = sr.Recognizer()
# with sr.AudioFile(AUDIO_FILE) as source:
#         audio = r.record(source)  # read the entire audio file

#         print("Transcription: " + r.recognize_google(audio))

#will need api_key for using recognize_google() function in SpeechRecognition, since it is a paid service!
#something like r.api_key = ....., and r.language = ....

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/
!git clone https://github.com/ggerganov/whisper.cpp
# %cd /content/whisper.cpp

!apt-get install g++

!bash ./models/download-ggml-model.sh base.en

!make

!make small.en

!ls

# !./main -m models/ggml-small.en.bin -f '/content/whisper.cpp/output16.wav'
# %%capture output
!./main -m models/ggml-small.en.bin -f '/content/output16.wav'
# print(output.stdout)

# # Save the captured output to a file
# with open('whisper_output.txt', 'w') as file:
#     file.write(output.stdout)

# ffmpeg -i /content/outputSong.mp3 -ar 16000 output16.wav

# !ffmpeg -i /content/outputSong.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output16.wav

#!./main -m models/ggml-small.en.bin -f output16.wav

#!ls

#!./main -m models/ggml-small.en.bin -t 6 -otxt -f output16.wav  #one mistake: i took threads = 6 considering that my CPU has 6 threads, so that will make an optimum spec, but I am currently using Google colab ka Processor, and not mine...and as far as i know...it has ig 3 threads only...so, confirm the same and accordingly select the most effective number of threads here!

"""# ***Step 3: Text-Audio Alignment ***"""

#!pip install timething

#!timething --help

#!pip install --upgrade setuptools wheel

# !pip cache purge
# !pip install timething

#!pip install tokenizers==0.10.1

#!git clone https://github.com/huggingface/transformers

# !cd transformers
# !pip install *

#!pip install tokenizers

#!pip install timething

#!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

#!pip install rustimport_jupyter

#!pip install timething

!ls

#!cd ./

#!ls

#!pip install pyproject.toml

#!rustc --version

#!pip --version

#!pip install setuptools_rust

#!rust --version

#!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# !pip install colab-ssh
# from colab_ssh import launch_ssh_cloudflared, get_cloudflared_tunnel_url

# # Launch an SSH server with acloudflared tunnel
# cloudflared_tunnel_url = launch_ssh_cloudflared(local_port=8080)

# # Get the URL of the SSH server
# ssh_url = get_cloudflared_tunnel_url()

# # Print the URL of the SSH server
# print(f"SSH server URL: {ssh_url}")

# # Connect to the SSH server using your preferred SSH client
# # and install the desired package

#!pip show colab_ssh

#!cd ./

#!rustup update

#!pip install wheel

#!python -m pip install --upgrade pip

#!pip install timething

#!pip install --upgrade pip setuptools

# timething align-long --audio-file fixtures/audio/keanu.mp3 --transcript-file fixtures/keanu.cleaned.txt --alignments-dir aligned --batch-size 10 --n-workers 5

# !python --version

# !sudo apt-get update -y
# !sudo apt-get install python3.11 python3.11-dev python3.11-distutils libpython3.11-dev

#!python3 --version

# !sudo apt-get install python3.8
# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8.1

#!update-alternatives --help

#!python --version

# !ln -sf /usr/bin/python3.8 /usr/bin/python

# !tensorflow --version

# !pip install tensorflow

# %tensorflow_versionÂ 1.x

# !pip show tensorflow

# !pip install jupyter_tensorboard

# %load_ext tensorboard

# %lsmagic

# %tensorflow_version 1.x

!pip install pydub numpy python_speech_features dtw-python

# import re

# def parse_output(output_file):
#     timestamps = []
#     transcripts = []

#     with open(output_file, 'r') as f:
#         for line in f:
#             match = re.match(r"\[(\d+:\d+:\d+\.\d+) --> (\d+:\d+:\d+\.\d+)\]\s+(.*)", line)
#             if match:
#                 start_time = match.group(1)
#                 end_time = match.group(2)
#                 text = match.group(3)
#                 timestamps.append((start_time, end_time))
#                 transcripts.append(text.strip())

#     return timestamps, transcripts

# import numpy as np
# from pydub import AudioSegment
# from python_speech_features import mfcc
# from dtw import dtw

# def align_transcription_with_audio(audio_file, timestamps, transcripts):
#     audio = AudioSegment.from_wav(audio_file)
#     rate = audio.frame_rate
#     audio_data = np.array(audio.get_array_of_samples())
#     mfcc_features = mfcc(audio_data, rate)

#     alignment_path = []

#     for start, end, text in zip(timestamps, transcripts):
#         start_time = float(start.replace("[", "").replace("]", ""))
#         end_time = float(end.replace("[", "").replace("]", ""))
#         segment_mfcc = mfcc_features[int(start_time * rate):int(end_time * rate)]
#         dist, _, _, _ = dtw(segment_mfcc.T, mfcc_features.T)
#         alignment_path.append((start, end, text))

#     return alignment_path

# if __name__ == "__main__":
#     # Replace 'output16.wav' and 'output.txt' with your audio file and Whisper output file paths
#     audio_file = 'output16.wav'
#     output_file = 'output.txt'

#     # Parse timestamps and transcripts from the output file
#     timestamps, transcripts = parse_output(output_file)

#     # Align transcripts with audio
#     alignment = align_transcription_with_audio(audio_file, timestamps, transcripts)

#     # Print aligned transcription
#     for start, end, text in alignment:
#         print(f"[{start} --> {end}] {text}")

import subprocess
import re
import numpy as np
from pydub import AudioSegment
from python_speech_features import mfcc
from dtw import dtw

def parse_output(output):
    lines = output.split('\n')
    timestamps = []
    transcripts = []

    # for line in output.split('\n'):
    for line in lines:
        if not line.strip():
            continue
        match = re.match(r'\[(.*?)\]   (.*)', line)
        if match:
            timestamps.append(match.group(1))
            transcripts.append(match.group(2))
        # match = re.match(r"\[(\d+:\d+:\d+\.\d+) --> (\d+:\d+:\d+\.\d+)\]\s+(.*)", line)
        # if match:
        #     start_time = match.group(1)
        #     end_time = match.group(2)
        #     text = match.group(3)
        #     timestamps.append((start_time, end_time))
        #     transcripts.append(text.strip())
        print(timestamps)
        print(transcripts)
    return timestamps, transcripts

def timestamp_to_seconds(timestamp):
    parts = timestamp.strip("[]").split(':')
    hours = int(parts[0])
    minutes = int(parts[1])
    seconds = float(parts[2])
    return hours * 3600 + minutes * 60 + seconds

def align_transcription_with_audio(audio_file, timestamps, transcripts):
    audio = AudioSegment.from_wav(audio_file)
    rate = audio.frame_rate
    print(f"rate {rate}")
    audio_data = np.array(audio.get_array_of_samples())
    print(f"audio_data {audio_data}")
    mfcc_features = mfcc(audio_data, rate)
    print(f"mfcc_features {mfcc_features}")

    # Calculate the MFCC frame rate
    hop_length = 160  # Default hop length for MFCC in python_speech_features
    mfcc_rate = rate / hop_length

    # mfcc_rate = len(mfcc_features) / (len(audio_data) / rate)  # MFCC features per second
    print(f"mfcc_rate {mfcc_rate}")
    print(f"length of mfcc_features {len(mfcc_features)}")

    alignment = []
    alignment_path = []

    # for start, end, text in zip(timestamps, transcripts):
    for i, (timestamp, text) in enumerate(zip(timestamps, transcripts)):
        start, end = timestamp.split(" --> ")
        start_time = timestamp_to_seconds(start)
        end_time = timestamp_to_seconds(end)

        print(f"Segment {i+1}: {text}")
        print(f"Start time: {start_time}, End time: {end_time}")
        print(f"Frame range: {int(start_time * mfcc_rate)} to {int(end_time * mfcc_rate)}")

        start_index = int(start_time * mfcc_rate)
        end_index = int(end_time * mfcc_rate)

        if start_index < 0 or end_index > len(mfcc_features):
            print(f"Skipping out of bounds segment MFCC for segment {i+1}")
            continue

        segment_mfcc = mfcc_features[start_index:end_index]
        print(f"length of segment_mfcc {len(segment_mfcc)}")

        # start_time = float(start.replace("[", "").replace("]", ""))
        # end_time = float(end.replace("[", "").replace("]", ""))

        # segment_mfcc = mfcc_features[int(start_time * rate):int(end_time * rate)]

        print(f"segment_mfcc {segment_mfcc}")
        print(f"segment_mfcc.T {segment_mfcc.T}")

        if segment_mfcc.T.shape[1] == 0:
           # Skip this segment as it does not contain enough information for MFCC extraction
           print(f"Skipping empty segment MFCC for segment {i+1}")
           continue

        # print(segment_mfcc.T.shape)
        # print(mfcc_features.T.shape)
        print(f"segment_mfcc.shape: {segment_mfcc.shape}")
        print(f"mfcc_features.shape: {mfcc_features.shape}")

        # dtw_result = dtw(segment_mfcc.T, mfcc_features.T)
        dtw_result = dtw(segment_mfcc, mfcc_features)
        print(type(dtw_result))
        print(dir(dtw_result))

        # dist, _, _, path = dtw(segment_mfcc.T, mfcc_features.T)
        dist = dtw_result.distance
        path = dtw_result.index1s
        # path = dtw_result.get_path()

        alignment.append((start_time, end_time, text))
        alignment_path.append(path)
        # alignment_path.append((start_time, end_time, text))

        print(f"Processed segment {i+1}: {text}, Start time: {start_time}, End time: {end_time}")

    return alignment, alignment_path

# if __name__ == "__main__":
#     # Run the 'main' command and capture its output
#     command_output = subprocess.check_output(["./main", "-m", "models/ggml-small.en.bin", "-f", "/content/whisper.cpp/output16.wav"], text=True)

#     # Parse timestamps and transcripts from the output
#     timestamps, transcripts = parse_output(command_output)

#     # Align transcripts with audio
#     alignment = align_transcription_with_audio("/content/whisper.cpp/output16.wav", timestamps, transcripts)

#     # Print aligned transcription
#     for start, end, text in alignment:
#         print(f"[{start} --> {end}] {text}")

output = """
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:22.840]   âª We're no strangers to love âª
[00:00:22.840 --> 00:00:27.040]   âª You know the rules and so do I âª
[00:00:27.040 --> 00:00:31.300]   âª I've filled commitments while I'm thinking of âª
[00:00:31.300 --> 00:00:35.280]   âª You wouldn't get this from any other guy âª
[00:00:35.280 --> 00:00:40.280]   âª I just wanna tell you how I'm feeling âª
[00:00:40.280 --> 00:00:43.160]   âª Gotta make you understand âª
[00:00:43.160 --> 00:00:45.240]   âª Never gonna give you up âª
[00:00:45.240 --> 00:00:47.380]   âª Never gonna let you down âª
[00:00:47.380 --> 00:00:51.520]   âª Never gonna run around and desert you âª
[00:00:51.520 --> 00:00:53.700]   âª Never gonna make you cry âª
[00:00:53.700 --> 00:00:55.800]   âª Never gonna say goodbye âª
[00:00:55.800 --> 00:01:00.800]   âª Never gonna tell a lie and hurt you âª
[00:01:00.800 --> 00:01:05.040]   âª We've known each other for so long âª
[00:01:05.040 --> 00:01:07.140]   âª Your heart's been aching us âª
[00:01:07.140 --> 00:01:09.340]   âª You're too shy to say it âª
[00:01:09.340 --> 00:01:13.520]   âª Inside we both know what's been going on âª
[00:01:13.520 --> 00:01:17.400]   âª We know the game and we're gonna play it âª
[00:01:17.400 --> 00:01:22.400]   âª And if you ask me how I'm feeling âª
[00:01:22.400 --> 00:01:25.360]   âª Don't tell me you're too glad to see âª
[00:01:25.360 --> 00:01:27.500]   âª Never gonna give you up âª
[00:01:27.500 --> 00:01:29.600]   âª Never gonna let you down âª
[00:01:29.600 --> 00:01:33.800]   âª Never gonna run around and desert you âª
[00:01:33.800 --> 00:01:35.940]   âª Never gonna make you cry âª
[00:01:35.940 --> 00:01:38.040]   âª Never gonna say goodbye âª
[00:01:38.040 --> 00:01:42.220]   âª Never gonna tell a lie and hurt you âª
[00:01:42.220 --> 00:01:44.340]   âª Never gonna give you up âª
[00:01:44.340 --> 00:01:46.480]   âª Never gonna let you down âª
[00:01:46.480 --> 00:01:50.680]   âª Never gonna run around and desert you âª
[00:01:50.680 --> 00:01:52.800]   âª Never gonna make you cry âª
[00:01:52.800 --> 00:01:54.920]   âª Never gonna say goodbye âª
[00:01:54.920 --> 00:01:59.860]   âª Never gonna tell a lie and hurt you âª
[00:01:59.860 --> 00:02:03.860]   âª Ooh, ooh, ooh, give you up âª
[00:02:03.860 --> 00:02:08.400]   âª Ooh, ooh, ooh, give you up âª
[00:02:08.400 --> 00:02:10.240]   âª Never gonna give, never gonna give âª
[00:02:10.240 --> 00:02:12.620]   âª Give you up âª
[00:02:12.620 --> 00:02:14.540]   âª Never gonna give, never gonna give âª
[00:02:14.540 --> 00:02:16.840]   âª Give you up âª
[00:02:16.840 --> 00:02:21.060]   âª We've known each other for so long âª
[00:02:21.060 --> 00:02:23.160]   âª Your heart's been aching us âª
[00:02:23.160 --> 00:02:25.360]   âª You're too shy to say it âª
[00:02:25.360 --> 00:02:29.560]   âª It's how we both know what's been going on âª
[00:02:29.560 --> 00:02:33.500]   âª We know the game and we're gonna play it âª
[00:02:33.500 --> 00:02:38.500]   âª I just wanna tell you how I feel, hey âª
[00:02:38.500 --> 00:02:41.440]   âª Gotta make you understand âª
[00:02:41.440 --> 00:02:43.580]   âª Never gonna give you up âª
[00:02:43.580 --> 00:02:45.680]   âª Never gonna let you down âª
[00:02:45.680 --> 00:02:49.820]   âª Never gonna run around and desert you âª
[00:02:49.820 --> 00:02:52.020]   âª Never gonna make you cry âª
[00:02:52.020 --> 00:02:54.180]   âª Never gonna say goodbye âª
[00:02:54.180 --> 00:02:58.360]   âª Never gonna tell a lie and hurt you âª
[00:02:58.360 --> 00:03:00.560]   âª Never gonna give you up âª
[00:03:00.560 --> 00:03:02.700]   âª Never gonna let you down âª
[00:03:02.700 --> 00:03:06.860]   âª Never gonna run around and desert you âª
[00:03:06.860 --> 00:03:08.960]   âª Never gonna make you cry âª
[00:03:08.960 --> 00:03:11.040]   âª Never gonna say goodbye âª
[00:03:11.040 --> 00:03:15.200]   âª Never gonna tell a lie and hurt you âª
[00:03:15.200 --> 00:03:17.380]   âª Never gonna give you up âª
[00:03:17.380 --> 00:03:19.580]   âª Never gonna let you down âª
[00:03:19.580 --> 00:03:21.980]   âª Never gonna run around and desert you âª
[00:03:21.980 --> 00:03:24.120]   âª Never gonna make you cry âª
[00:03:24.120 --> 00:03:26.720]   âª Never gonna say goodbye âª
[00:03:26.720 --> 00:03:31.060]   âª Never gonna tell a lie and hurt you âª
"""

timestamps, transcripts = parse_output(output)
# alignment = align_transcription_with_audio("/content/whisper.cpp/output16.wav", timestamps, transcripts)
alignment, alignment_path = align_transcription_with_audio("/content/output16.wav", timestamps, transcripts)

# Print the alignment and alignment path
print("Alignment:")
for segment in alignment:
    print(segment)

print("Alignment Path:")
for path in alignment_path:
    print(path)

"""# *ALSO EXPLORE HOW TO USE AENEAS (WHICH IS ALSO BASED ON THIS DTW ALGO) FOR THE SAME ABOVE TASK!!!*

**Verifying if the text (Transcript) - audio alignment has been done properly or not:**
"""

# !pip install simpleaudio

# import simpleaudio as sa

# def play_audio_segment(audio_file, start_time, end_time):
#     audio = AudioSegment.from_wav(audio_file)
#     segment = audio[start_time*1000:end_time*1000]  # Convert seconds to milliseconds
#     play_obj = sa.play_buffer(segment.raw_data, num_channels=segment.channels, bytes_per_sample=segment.sample_width, sample_rate=segment.frame_rate)
#     play_obj.wait_done()

# # Example usage:
# for start_time, end_time, text in alignment:
#     print(f"Playing segment: {text}")
#     play_audio_segment("/content/output16.wav", start_time, end_time)
#     input("Press Enter to play the next segment...")

# # This will play each audio segment and prompt you to press Enter to play the next segment.

!pip install IPython

import os
import tempfile
import time
from pydub import AudioSegment
from IPython.display import Audio, display

def play_audio_segment(audio_file, start_time, end_time):
    audio = AudioSegment.from_wav(audio_file)
    segment = audio[start_time * 1000:end_time * 1000]  # Convert seconds to milliseconds

    # Create a temporary file to save the segment
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
        segment.export(temp_file.name, format="wav")
        temp_file.close()
        display(Audio(temp_file.name))
        os.remove(temp_file.name)

# Example usage:
for start_time, end_time, text in alignment:
    print(f"Playing segment: {text}")
    play_audio_segment("/content/output16.wav", start_time, end_time)
    time.sleep(5)  # Wait for 5 seconds before playing the next segment
    # input("Press Enter to play the next segment...")

import numpy as np
from pydub import AudioSegment
from pydub.silence import split_on_silence
import os

def load_audio(audio_file):
    return AudioSegment.from_wav(audio_file)

def split_audio(audio, min_silence_len=500, silence_thresh=-40):
    print(audio)
    chunks = split_on_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)
    return chunks

def chunk_audio(audio, chunk_duration=15000):
    chunks = []
    for i in range(0, len(audio), chunk_duration):
        chunks.append(audio[i:i+chunk_duration])
        print(chunks)
    return chunks

def semantic_chunking(audio_chunks, text_segments):
    semantic_chunks = []
    for audio_chunk, text_segment in zip(audio_chunks, text_segments):
        # Split text segment into sentences
        sentences = sent_tokenize(text_segment)
        num_sentences = len(sentences)

        if num_sentences == 0:
            continue

        # Calculate average duration per sentence
        avg_duration_per_sentence = len(audio_chunk) / num_sentences

        # Chunk audio based on sentence boundaries
        start_time = 0
        for sentence in sentences:
            sentence_duration = len(sentence) / len(text_segment) * len(audio_chunk)
            end_time = start_time + sentence_duration
            semantic_chunks.append((audio_chunk[start_time:end_time], sentence))
            start_time = end_time

    return semantic_chunks

def save_semantic_chunks(semantic_chunks, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    for i, (audio_chunk, text_segment) in enumerate(semantic_chunks):
        audio_chunk.export(os.path.join(output_dir, f"audio_chunk_{i}.wav"), format="wav")
        with open(os.path.join(output_dir, f"text_segment_{i}.txt"), "w") as f:
            f.write(text_segment)

# Replace these paths with the paths to your audio and text data
audio_file = "/content/output16.wav"
# text_file = "/content/text.txt"

audio = load_audio(audio_file)
audio_chunks = split_audio(audio)
print(audio_chunks)
audio_chunks = [chunk for chunk in audio_chunks if len(chunk) <= 15000]  # Filter out chunks longer than 15 seconds

# with open(text_file, "r") as f:
#     text = f.read()

print(audio_chunks)

semantic_chunks = semantic_chunking(audio_chunks, transcripts)
# Save semantic chunks
output_dir = "/content/semantic_chunks"
print(len(semantic_chunks))
print(semantic_chunks)
save_semantic_chunks(semantic_chunks, output_dir)

!pip install librosa numpy

import librosa
import numpy as np

# Load audio file
audio_file = "/content/output16.wav"
audio, sr = librosa.load(audio_file, sr=None)

# Beat detection
tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)

# Calculate segment boundaries based on beats
segment_boundaries = librosa.frames_to_time(beats, sr=sr)
segment_boundaries = np.concatenate([[0], segment_boundaries, [len(audio)/sr]])

# Semantic chunking
semantic_chunks = []
for i in range(len(segment_boundaries) - 1):
    start_time = segment_boundaries[i]
    end_time = segment_boundaries[i + 1]
    audio_chunk = audio[int(start_time * sr):int(end_time * sr)]

    # Find lyrics corresponding to the audio chunk
    corresponding_alignments = []
    for item in alignment:
        alignment_start, alignment_end, alignment_text = item
        if start_time <= alignment_start < end_time or start_time < alignment_end <= end_time:
            corresponding_alignments.append(item)

    semantic_chunks.append((audio_chunk, corresponding_alignments))

# Filter out chunks longer than 15 seconds
semantic_chunks = [(audio_chunk, alignment_items) for audio_chunk, alignment_items in semantic_chunks if len(audio_chunk) / sr < 15]

# Process semantic chunks
for i, (audio_chunk, alignment_items) in enumerate(semantic_chunks):
    print(f"Segment {i + 1}:")
    print(f"Audio Duration: {len(audio_chunk) / sr} seconds")
    print("Transcript Alignments:")
    for alignment_start, alignment_end, alignment_text in alignment_items:
        print(f"- {alignment_start} - {alignment_end} - {alignment_text}")
    print()

import numpy as np
import soundfile as sf

# Assuming we have transcript alignments in the format: (start_time, end_time, text)

# Load audio file
audio_file = "/content/output16.wav"
audio, sr = librosa.load(audio_file, sr=None)

# Split audio into chunks of approximately 15 seconds each
chunk_duration = 15  # in seconds
num_chunks = int(np.ceil(len(audio) / sr / chunk_duration))
chunk_size = int(len(audio) / num_chunks)
# chunk_size = int(chunk_duration * sr)         //whats this logic...check once later on!!

# Create output directories
output_audio_dir = "/content/audio_chunks"
output_transcript_file = "/content/transcript_chunks.txt"
os.makedirs(output_audio_dir, exist_ok=True)

# Semantic chunking
semantic_chunks = []
for i in range(num_chunks):
    start_index = i * chunk_size
    end_index = min((i + 1) * chunk_size, len(audio))
    audio_chunk = audio[start_index:end_index]

    # Find transcript alignments corresponding to the audio chunk
    corresponding_alignments = []
    for item in alignment:
        alignment_start, alignment_end, alignment_text = item
        if start_index / sr <= alignment_start < end_index / sr or start_index / sr < alignment_end <= end_index / sr:
            corresponding_alignments.append(item)

    if corresponding_alignments:
        semantic_chunks.append((audio_chunk, corresponding_alignments))

       # Save audio chunk
        audio_chunk_filename = os.path.join(output_audio_dir, f"chunk_{i + 1}.wav")
        sf.write(audio_chunk_filename, audio_chunk, sr)

# # Filter out chunks with no transcript alignments
# semantic_chunks = [(audio_chunk, alignment_items) for audio_chunk, alignment_items in semantic_chunks if alignment_items]

# Save transcript alignments to a text file
with open(output_transcript_file, "w") as f:
    for i, (audio_chunk, alignment_items) in enumerate(semantic_chunks):
        f.write(f"Segment {i + 1}:\n")
        f.write(f"Audio Filename: chunk_{i + 1}.wav\n")
        f.write(f"Audio Duration: {len(audio_chunk) / sr} seconds\n")
        f.write("Transcript Alignments:\n")
        for alignment_start, alignment_end, alignment_text in alignment_items:
            f.write(f"- {alignment_start:.2f} - {alignment_end:.2f} - {alignment_text}\n")
        f.write("\n")

print("Audio chunks and transcripts have been saved.")

# Process semantic chunks
for i, (audio_chunk, corresponding_alignments) in enumerate(semantic_chunks):
    print(f"Segment {i + 1}:")
    print(f"Audio Duration: {len(audio_chunk) / sr} seconds")
    print("Transcript Alignments:")
    for alignment_start, alignment_end, alignment_text in corresponding_alignments:
        print(f"- {alignment_start} - {alignment_end} - {alignment_text}")
    print()

