{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3s1QKqHVLo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e863eef-7573-4a28-f79d-edff3ca875d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m51.2/57.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.11.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=4f390840188c929d5ea4a7bd6f22dd7e7537c8ae6561fdca2c7ca260f73dee58\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: pydub, ffmpeg, pytube, jedi\n",
            "Successfully installed ffmpeg-1.4 jedi-0.19.1 pydub-0.25.1 pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube moviepy pydub ffmpeg IPython librosa numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "89eFoeWMV0Yi",
        "outputId": "2a998827-9bab-45e3-8b65-134156567731"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import *\n",
        "\n",
        "# Enter the YouTube video URL\n",
        "url = 'https://www.youtube.com/watch?v=Sby1uJ_NFIY'\n",
        "\n",
        "# Download the video\n",
        "yt = YouTube(url)\n",
        "stream = yt.streams.get_highest_resolution()\n",
        "stream.download()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMjbJ6VqZ_Lc",
        "outputId": "3db853ab-eea2-446f-f3a8-854f347a2a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/Sarvam_Audio_output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Extract the audio\n",
        "video = VideoFileClip('/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4')\n",
        "audio = video.audio\n",
        "audio.write_audiofile('/content/Sarvam_Audio_output.mp3')\n",
        "\n",
        "# Delete the downloaded video file\n",
        "os.remove('/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY-083tTaZq4",
        "outputId": "c332c1c0-f296-431a-911a-bbc6b4edfb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mp3, from '/content/Sarvam_Audio_output.mp3':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:26:15.05, start: 0.025057, bitrate: 128 kb/s\n",
            "  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 128 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'Sarvam_output16000.wav':\n",
            "  Metadata:\n",
            "    ISFT            : Lavf58.76.100\n",
            "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=   49219kB time=00:26:15.00 bitrate= 256.0kbits/s speed= 524x    \n",
            "video:0kB audio:49219kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000155%\n"
          ]
        }
      ],
      "source": [
        "!ffmpeg -i /content/Sarvam_Audio_output.mp3 -ar 16000 -ac 1 -c:a pcm_s16le Sarvam_output16000.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVNwo9aYbT9j",
        "outputId": "9c6204ff-2b6b-400c-e0fe-b268530da95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 8387, done.\u001b[K\n",
            "remote: Counting objects: 100% (2982/2982), done.\u001b[K\n",
            "remote: Compressing objects: 100% (448/448), done.\u001b[K\n",
            "remote: Total 8387 (delta 2727), reused 2625 (delta 2532), pack-reused 5405\u001b[K\n",
            "Receiving objects: 100% (8387/8387), 12.32 MiB | 22.81 MiB/s, done.\n",
            "Resolving deltas: 100% (5541/5541), done.\n",
            "/content/whisper.cpp\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/ggerganov/whisper.cpp\n",
        "%cd /content/whisper.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnaqSMbqbZBy",
        "outputId": "9d5f33a5-2e6a-4801-aba7-6e8000246bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "g++ set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install g++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI8BreUUbd17",
        "outputId": "43ebe945-aa3e-422a-95f0-c81b1af8bdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ggml model base.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "ggml-base.en.bin    100%[===================>] 141.11M   320MB/s    in 0.4s    \n",
            "Done! Model 'base.en' saved in '/content/whisper.cpp/models/ggml-base.en.bin'\n",
            "You can now use it like this:\n",
            "\n",
            "  $ ./main -m /content/whisper.cpp/models/ggml-base.en.bin -f samples/jfk.wav\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash ./models/download-ggml-model.sh base.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Solmb9uqbfVu",
        "outputId": "00db56af-ad8a-44c1-f72f-6059a99ce3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw   -c ggml.c -o ggml.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw   -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw   -c ggml-backend.c -o ggml-backend.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw   -c ggml-quants.c -o ggml-quants.o\n",
            "c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw -c whisper.cpp -o whisper.o\n",
            "c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw examples/main/main.cpp examples/common.cpp examples/common-ggml.cpp examples/grammar-parser.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o main \n",
            "./main -h\n",
            "\n",
            "usage: ./main [options] file0.wav file1.wav ...\n",
            "\n",
            "options:\n",
            "  -h,        --help              [default] show this help message and exit\n",
            "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
            "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
            "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
            "  -on N,     --offset-n N        [0      ] segment index offset\n",
            "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
            "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
            "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
            "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
            "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
            "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
            "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
            "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
            "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
            "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
            "  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n",
            "  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n",
            "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
            "  -tr,       --translate         [false  ] translate from source language to english\n",
            "  -di,       --diarize           [false  ] stereo audio diarization\n",
            "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
            "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
            "  -otxt,     --output-txt        [false  ] output result in a text file\n",
            "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
            "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
            "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
            "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
            "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
            "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
            "  -oj,       --output-json       [false  ] output result in a JSON file\n",
            "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
            "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
            "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
            "  -ps,       --print-special     [false  ] print special tokens\n",
            "  -pc,       --print-colors      [false  ] print colors\n",
            "  -pp,       --print-progress    [false  ] print progress\n",
            "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
            "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
            "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
            "             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n",
            "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
            "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
            "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
            "  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n",
            "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
            "  -ng,       --no-gpu            [false  ] disable GPU\n",
            "  -fa,       --flash-attn        [false  ] flash attention\n",
            "  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n",
            "  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n",
            "  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n",
            "  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n",
            "\n",
            "c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw examples/bench/bench.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o bench \n",
            "c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw examples/quantize/quantize.cpp examples/common.cpp examples/common-ggml.cpp examples/grammar-parser.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o quantize \n",
            "c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw examples/server/server.cpp examples/common.cpp examples/common-ggml.cpp examples/grammar-parser.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o server  \n"
          ]
        }
      ],
      "source": [
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSKUHL9ZbhQq",
        "outputId": "869a65f6-fcba-4cf8-fb38-038bafe7b3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -msse3 -mssse3 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "bash ./models/download-ggml-model.sh small.en\n",
            "Downloading ggml model small.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "ggml-small.en.bin   100%[===================>] 465.02M   323MB/s    in 1.4s    \n",
            "Done! Model 'small.en' saved in '/content/whisper.cpp/models/ggml-small.en.bin'\n",
            "You can now use it like this:\n",
            "\n",
            "  $ ./main -m /content/whisper.cpp/models/ggml-small.en.bin -f samples/jfk.wav\n",
            "\n",
            "\n",
            "===============================================\n",
            "Running small.en on all samples in ./samples ...\n",
            "===============================================\n",
            "\n",
            "----------------------------------------------\n",
            "[+] Running small.en on samples/jfk.wav ... (run 'ffplay samples/jfk.wav' to listen)\n",
            "----------------------------------------------\n",
            "\n",
            "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-small.en.bin'\n",
            "whisper_init_with_params_no_state: use gpu    = 1\n",
            "whisper_init_with_params_no_state: flash attn = 0\n",
            "whisper_init_with_params_no_state: gpu_device = 0\n",
            "whisper_init_with_params_no_state: dtw        = 0\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51864\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 768\n",
            "whisper_model_load: n_audio_head  = 12\n",
            "whisper_model_load: n_audio_layer = 12\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 768\n",
            "whisper_model_load: n_text_head   = 12\n",
            "whisper_model_load: n_text_layer  = 12\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 3 (small)\n",
            "whisper_model_load: adding 1607 extra tokens\n",
            "whisper_model_load: n_langs       = 99\n",
            "whisper_model_load:      CPU total size =   487.00 MB\n",
            "whisper_model_load: model size    =  487.00 MB\n",
            "whisper_init_state: kv self size  =   56.62 MB\n",
            "whisper_init_state: kv cross size =   56.62 MB\n",
            "whisper_init_state: kv pad  size  =    4.72 MB\n",
            "whisper_init_state: compute buffer (conv)   =   22.54 MB\n",
            "whisper_init_state: compute buffer (encode) =  280.20 MB\n",
            "whisper_init_state: compute buffer (cross)  =    6.31 MB\n",
            "whisper_init_state: compute buffer (decode) =   97.40 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0\n",
            "\n",
            "main: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 2 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
            "\n",
            "\n",
            "[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n",
            "\n",
            "\n",
            "whisper_print_timings:     load time =   347.54 ms\n",
            "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
            "whisper_print_timings:      mel time =    47.64 ms\n",
            "whisper_print_timings:   sample time =   159.28 ms /   139 runs (    1.15 ms per run)\n",
            "whisper_print_timings:   encode time = 21078.73 ms /     1 runs (21078.73 ms per run)\n",
            "whisper_print_timings:   decode time =   166.65 ms /     3 runs (   55.55 ms per run)\n",
            "whisper_print_timings:   batchd time =  3834.37 ms /   132 runs (   29.05 ms per run)\n",
            "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:    total time = 25711.57 ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!make small.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88IX9vUobkDS",
        "outputId": "bec3c801-fade-4c95-ff02-7de54f3fddf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-small.en.bin'\n",
            "whisper_init_with_params_no_state: use gpu    = 1\n",
            "whisper_init_with_params_no_state: flash attn = 0\n",
            "whisper_init_with_params_no_state: gpu_device = 0\n",
            "whisper_init_with_params_no_state: dtw        = 0\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51864\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 768\n",
            "whisper_model_load: n_audio_head  = 12\n",
            "whisper_model_load: n_audio_layer = 12\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 768\n",
            "whisper_model_load: n_text_head   = 12\n",
            "whisper_model_load: n_text_layer  = 12\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 3 (small)\n",
            "whisper_model_load: adding 1607 extra tokens\n",
            "whisper_model_load: n_langs       = 99\n",
            "whisper_model_load:      CPU total size =   487.00 MB\n",
            "whisper_model_load: model size    =  487.00 MB\n",
            "whisper_init_state: kv self size  =   56.62 MB\n",
            "whisper_init_state: kv cross size =   56.62 MB\n",
            "whisper_init_state: kv pad  size  =    4.72 MB\n",
            "whisper_init_state: compute buffer (conv)   =   22.54 MB\n",
            "whisper_init_state: compute buffer (encode) =  280.20 MB\n",
            "whisper_init_state: compute buffer (cross)  =    6.31 MB\n",
            "whisper_init_state: compute buffer (decode) =   97.40 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0\n",
            "\n",
            "main: processing '/content/Sarvam_output16000.wav' (25200160 samples, 1575.0 sec), 2 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
            "\n",
            "\n",
            "[00:00:00.000 --> 00:00:02.120]   >> Congratulations to you, Mr. Raghavan, for that.\n",
            "[00:00:02.120 --> 00:00:03.400]   Thank you so much for joining us.\n",
            "[00:00:03.400 --> 00:00:04.120]   Over to you.\n",
            "[00:00:04.120 --> 00:00:09.320]   >> Hi, everybody.\n",
            "[00:00:09.320 --> 00:00:10.120]   How are you?\n",
            "[00:00:10.120 --> 00:00:13.600]   Okay, I am not hearing this at all.\n",
            "[00:00:13.600 --> 00:00:17.000]   It's like a post-lunch energy downer or something.\n",
            "[00:00:17.000 --> 00:00:18.780]   Let's hear it.\n",
            "[00:00:18.780 --> 00:00:20.880]   Are you guys awake?\n",
            "[00:00:20.880 --> 00:00:22.300]   All right.\n",
            "[00:00:22.300 --> 00:00:26.880]   You better be, because we have a superstar guest here.\n",
            "[00:00:26.880 --> 00:00:29.400]   You heard the $41 million.\n",
            "[00:00:29.400 --> 00:00:33.680]   And I didn't hear, honestly, anything that she said after that.\n",
            "[00:00:33.680 --> 00:00:36.920]   So, we're going to ask for about $40 million from him\n",
            "[00:00:36.920 --> 00:00:38.120]   by the end of this conversation.\n",
            "[00:00:38.120 --> 00:00:40.600]   Okay? But let's get started.\n",
            "[00:00:40.600 --> 00:00:43.740]   I want to introduce Vivek and Pratyush,\n",
            "[00:00:43.740 --> 00:00:44.960]   his co-founder, who's not here.\n",
            "[00:00:44.960 --> 00:00:51.020]   We wanted to start with playing a video of what OpenHati does.\n",
            "[00:00:51.020 --> 00:00:53.900]   I encourage all of you to go to the website,\n",
            "[00:00:53.900 --> 00:00:55.360]   sevron.ai, and check it out.\n",
            "[00:00:55.360 --> 00:00:58.220]   But let me start by introducing Vivek.\n",
            "[00:00:58.560 --> 00:01:02.480]   Vivek is a dear friend, and he's very, very modest,\n",
            "[00:01:02.480 --> 00:01:04.360]   one of the most modest guys that I know.\n",
            "[00:01:04.360 --> 00:01:06.820]   But his personal journey, Vivek, you've been --\n",
            "[00:01:06.820 --> 00:01:09.160]   you got a PhD from Carnegie Mellon.\n",
            "[00:01:09.160 --> 00:01:11.920]   You started and sold a company to Magma.\n",
            "[00:01:11.920 --> 00:01:14.240]   And Vivek and I moved back to India from --\n",
            "[00:01:14.240 --> 00:01:16.580]   we were both in the valley on the same day, actually.\n",
            "[00:01:16.580 --> 00:01:19.520]   And you've been in India for the last 16 years.\n",
            "[00:01:19.520 --> 00:01:24.440]   And what most people don't know is your journey at Adhar.\n",
            "[00:01:24.440 --> 00:01:27.800]   He spent 13 years selflessly at Adhar.\n",
            "[00:01:28.200 --> 00:01:29.780]   Nobody would have heard of him.\n",
            "[00:01:29.780 --> 00:01:35.880]   But he was a pioneering technology visionary behind Adhar,\n",
            "[00:01:35.880 --> 00:01:37.320]   which we all take for granted today.\n",
            "[00:01:37.320 --> 00:01:40.120]   So please give it out.\n",
            "[00:01:40.120 --> 00:01:43.160]   So honestly, when people --\n",
            "[00:01:43.160 --> 00:01:44.440]   when I think of selfless service,\n",
            "[00:01:44.440 --> 00:01:47.960]   truly selfless service, I always think of Vivek.\n",
            "[00:01:47.960 --> 00:01:50.820]   And since then, he also was at AI for Bharat,\n",
            "[00:01:50.820 --> 00:01:52.360]   which we're going to touch on,\n",
            "[00:01:52.360 --> 00:01:54.740]   where he met Pratyush's other co-founder.\n",
            "[00:01:54.740 --> 00:01:57.940]   Pratyush had a PhD from ETH at Zurich.\n",
            "[00:01:58.220 --> 00:02:00.420]   He was at IBM Research.\n",
            "[00:02:00.420 --> 00:02:02.500]   He was at Microsoft Research, playing a key role\n",
            "[00:02:02.500 --> 00:02:06.020]   and a faculty at IIT Madras and at AI for Bharat.\n",
            "[00:02:06.020 --> 00:02:08.800]   So that's a little brief introduction about them.\n",
            "[00:02:08.800 --> 00:02:10.940]   These guys are modest, modest engineers.\n",
            "[00:02:10.940 --> 00:02:13.320]   So they don't toot their own horn.\n",
            "[00:02:13.320 --> 00:02:17.060]   So forgive me for tooting their horn in this case.\n",
            "[00:02:17.060 --> 00:02:22.040]   But let's jump right in about the money, funding.\n",
            "[00:02:22.040 --> 00:02:23.720]   Forty-one million bucks, man.\n",
            "[00:02:23.720 --> 00:02:25.740]   That's a lot of money, right?\n",
            "[00:02:25.740 --> 00:02:28.560]   Every entrepreneur here is saying, \"What the hell did these guys do?\"\n",
            "[00:02:28.560 --> 00:02:31.500]   What did the investor see to write such a big check?\n",
            "[00:02:31.500 --> 00:02:38.000]   >> No, I think it's a new trend of what's going on in India.\n",
            "[00:02:38.000 --> 00:02:39.920]   I think that for the very first time,\n",
            "[00:02:39.920 --> 00:02:43.120]   I think the investors have looked at, you know,\n",
            "[00:02:43.120 --> 00:02:46.020]   let's try and build something deep tech out of the country.\n",
            "[00:02:46.020 --> 00:02:48.900]   And let's try to figure out how to build something\n",
            "[00:02:48.900 --> 00:02:50.920]   as a foundational technology out of the country.\n",
            "[00:02:50.920 --> 00:02:53.540]   And that's really what's really exciting, you know?\n",
            "[00:02:53.540 --> 00:03:00.260]   And I think that about, you know, as Bala was mentioning\n",
            "[00:03:00.260 --> 00:03:03.860]   for the past 15 years, I've been kind of working in kind of,\n",
            "[00:03:03.860 --> 00:03:08.420]   you know, both digital public infrastructure and kind\n",
            "[00:03:08.420 --> 00:03:10.560]   of nonprofit kind of things.\n",
            "[00:03:10.560 --> 00:03:13.700]   And when this whole thing of generative AI came about,\n",
            "[00:03:13.700 --> 00:03:16.060]   I said, you know, we said, \"Okay,\n",
            "[00:03:16.060 --> 00:03:19.140]   how can I actually make a difference in this space?\"\n",
            "[00:03:19.140 --> 00:03:22.420]   And I said, \"Maybe this is the opportunity to actually come\n",
            "[00:03:22.420 --> 00:03:27.140]   out and really build something, you know, and the only way\n",
            "[00:03:27.140 --> 00:03:29.020]   that we realize that you can do it is actually\n",
            "[00:03:29.020 --> 00:03:32.660]   in the private sector.\"\n",
            "[00:03:32.660 --> 00:03:35.500]   And I think that's -- and then we went out there and we said,\n",
            "[00:03:35.500 --> 00:03:38.140]   \"We want to build something,\" which is a continuation, right?\n",
            "[00:03:38.140 --> 00:03:41.260]   I mean, and fundamentally, the question is the reason\n",
            "[00:03:41.260 --> 00:03:43.780]   of what we want to do at Servum AI is we want\n",
            "[00:03:43.780 --> 00:03:48.280]   to basically make generative AI available and accessible\n",
            "[00:03:48.280 --> 00:03:49.660]   to the people in the country.\n",
            "[00:03:49.660 --> 00:03:51.340]   And that's the intent.\n",
            "[00:03:51.340 --> 00:03:53.420]   And when we said that we want to do this,\n",
            "[00:03:53.420 --> 00:03:56.300]   there was a resonance in the investment community.\n",
            "[00:03:56.300 --> 00:03:59.940]   And I think it's a responsibility to really to show\n",
            "[00:03:59.940 --> 00:04:02.700]   that something like this can be built out of India.\n",
            "[00:04:02.700 --> 00:04:06.660]   So we see that as confidence and a responsibility.\n",
            "[00:04:06.660 --> 00:04:09.620]   And I also hope it's a trend that, you know,\n",
            "[00:04:09.620 --> 00:04:12.300]   that there are many more people like us who are backed.\n",
            "[00:04:12.300 --> 00:04:16.700]   Because if you look at it, maybe it's a large number in a,\n",
            "[00:04:16.700 --> 00:04:19.620]   you know, in the Indian context, but in the global context,\n",
            "[00:04:19.920 --> 00:04:21.440]   I think there is just -- there should be many,\n",
            "[00:04:21.440 --> 00:04:24.660]   many more entrepreneurs who are backed to do things in India.\n",
            "[00:04:24.660 --> 00:04:26.540]   >> Yeah, I'm going to come back\n",
            "[00:04:26.540 --> 00:04:27.560]   to the many more entrepreneurs.\n",
            "[00:04:27.560 --> 00:04:30.880]   I'm obviously going to ask you about Babish's krutram.\n",
            "[00:04:30.880 --> 00:04:33.520]   So we're going to come back to that question.\n",
            "[00:04:33.520 --> 00:04:36.460]   But again, $41 million.\n",
            "[00:04:36.460 --> 00:04:39.800]   I mean, all of what you said, you know, $2 million, you know,\n",
            "[00:04:39.800 --> 00:04:43.060]   that's a good amount of money for a startup which, you know,\n",
            "[00:04:43.060 --> 00:04:44.440]   which has not yet built anything.\n",
            "[00:04:44.440 --> 00:04:46.320]   What are you going to do with all this money?\n",
            "[00:04:47.360 --> 00:04:50.260]   >> I can call the problem.\n",
            "[00:04:50.260 --> 00:04:52.020]   I can have a perfect solution for the problem.\n",
            "[00:04:52.020 --> 00:04:54.380]   >> I think in the last week, I've got lots of calls\n",
            "[00:04:54.380 --> 00:04:57.980]   of lots of people telling me how I can -- no, but --\n",
            "[00:04:57.980 --> 00:04:59.320]   >> I know you first, okay?\n",
            "[00:04:59.320 --> 00:05:00.940]   I'll be landed in the country the same day.\n",
            "[00:05:00.940 --> 00:05:01.880]   I'm on the front of the queue.\n",
            "[00:05:01.880 --> 00:05:07.840]   >> No, but honestly, I think the key thing in this is\n",
            "[00:05:07.840 --> 00:05:09.440]   to putting together an amazing team.\n",
            "[00:05:09.440 --> 00:05:12.100]   And we actually have an amazing team, but we believe\n",
            "[00:05:12.100 --> 00:05:15.140]   that it is talent that will drive this kind of thing.\n",
            "[00:05:15.140 --> 00:05:18.020]   And so it is to get key talent.\n",
            "[00:05:18.020 --> 00:05:19.720]   And of course, the other thing is compute.\n",
            "[00:05:19.720 --> 00:05:23.200]   This is extremely expensive compute-wise\n",
            "[00:05:23.200 --> 00:05:25.200]   to actually do these kinds of things.\n",
            "[00:05:25.200 --> 00:05:27.940]   And I think that those are the two primary things\n",
            "[00:05:27.940 --> 00:05:30.220]   that, you know, we'd use this for.\n",
            "[00:05:30.220 --> 00:05:34.900]   >> Okay. I'm computing in my own head as an entrepreneur.\n",
            "[00:05:34.900 --> 00:05:37.280]   Talent, okay, you have like 20, 15 people.\n",
            "[00:05:37.280 --> 00:05:38.860]   How much are you paying these guys?\n",
            "[00:05:38.860 --> 00:05:41.580]   But okay, well, we won't touch on that.\n",
            "[00:05:41.580 --> 00:05:43.720]   But let's talk about what you guys actually built.\n",
            "[00:05:43.720 --> 00:05:46.380]   What does OpenHati, how would you explain OpenHati\n",
            "[00:05:46.380 --> 00:05:48.660]   to many people here who might not have known about it?\n",
            "[00:05:48.660 --> 00:05:53.700]   >> So I think OpenHati is -- so first of all, right, we come from --\n",
            "[00:05:53.700 --> 00:05:57.200]   I personally come from the open source ecosystem.\n",
            "[00:05:57.200 --> 00:06:00.060]   And we -- and also from the DPI ecosystem.\n",
            "[00:06:00.060 --> 00:06:02.560]   So we believe that for this to work,\n",
            "[00:06:02.560 --> 00:06:05.320]   we need the ecosystem to be successful.\n",
            "[00:06:05.320 --> 00:06:09.040]   And as a result of that, one of the first things we did was, hey,\n",
            "[00:06:09.040 --> 00:06:11.880]   there are these open source large language models\n",
            "[00:06:11.880 --> 00:06:12.700]   that exist, right?\n",
            "[00:06:12.700 --> 00:06:15.800]   I mean, everybody knows about the Lama family from META.\n",
            "[00:06:15.800 --> 00:06:18.500]   They also -- and there are others like Mistral.\n",
            "[00:06:18.500 --> 00:06:21.960]   There are a bunch of open source, you know,\n",
            "[00:06:21.960 --> 00:06:23.200]   large language models.\n",
            "[00:06:23.200 --> 00:06:25.940]   And then we said, is there any way\n",
            "[00:06:25.940 --> 00:06:28.920]   that take an existing open source model\n",
            "[00:06:28.920 --> 00:06:30.820]   and teach it language skills, right?\n",
            "[00:06:30.820 --> 00:06:34.580]   I mean, and that is really the, you know, what we decide --\n",
            "[00:06:34.580 --> 00:06:37.080]   what we said that can we do something like that?\n",
            "[00:06:37.080 --> 00:06:41.540]   And is this a, you know, relatively frugal way of actually,\n",
            "[00:06:41.540 --> 00:06:44.940]   you know, making models, you know,\n",
            "[00:06:44.940 --> 00:06:47.520]   work in diverse languages?\n",
            "[00:06:47.520 --> 00:06:49.360]   Because the truth is still today.\n",
            "[00:06:49.360 --> 00:06:52.340]   I mean, if you look at the amount of data\n",
            "[00:06:52.340 --> 00:06:54.900]   and knowledge, it is still -- English dominates these things.\n",
            "[00:06:54.900 --> 00:06:57.560]   And I think that how do you actually take\n",
            "[00:06:57.560 --> 00:06:59.820]   and make it understand Indian language,\n",
            "[00:06:59.820 --> 00:07:02.880]   understand Indian context, and all of those things in --\n",
            "[00:07:02.880 --> 00:07:05.040]   actually in an efficient way?\n",
            "[00:07:05.040 --> 00:07:07.120]   And therefore, this was an attempt to do that.\n",
            "[00:07:07.480 --> 00:07:11.800]   And it's an open happy is, you know, is currently based\n",
            "[00:07:11.800 --> 00:07:13.680]   on the LAMA 7 billion model.\n",
            "[00:07:13.680 --> 00:07:16.760]   But we'll be releasing many more models in different languages,\n",
            "[00:07:16.760 --> 00:07:21.960]   different sizes, and things like that as part of this series.\n",
            "[00:07:21.960 --> 00:07:24.960]   And of course, you know, we will be building further models\n",
            "[00:07:24.960 --> 00:07:27.940]   on those and doing other things to actually --\n",
            "[00:07:27.940 --> 00:07:30.020]   and we'll also have endpoints that people can use.\n",
            "[00:07:30.020 --> 00:07:32.120]   So therefore, it's not -- it's definitely, you know,\n",
            "[00:07:32.120 --> 00:07:35.340]   something that people can use to things.\n",
            "[00:07:35.340 --> 00:07:40.140]   And that's the essence of what this open happy is.\n",
            "[00:07:40.140 --> 00:07:41.820]   >> So what does it mean to people in the audience here\n",
            "[00:07:41.820 --> 00:07:44.600]   who are either doing their own start-ups or a business\n",
            "[00:07:44.600 --> 00:07:49.200]   or developers, how should they look at OpenAI?\n",
            "[00:07:49.200 --> 00:07:51.620]   Sorry, Sarban, not OpenAI?\n",
            "[00:07:51.620 --> 00:07:53.320]   >> No, yeah, no, no.\n",
            "[00:07:53.320 --> 00:07:57.660]   I think the way you look at it is that one of the important things\n",
            "[00:07:57.660 --> 00:08:00.640]   that we are doing is we're not just building models.\n",
            "[00:08:00.640 --> 00:08:03.920]   We are also going to be building a platform,\n",
            "[00:08:04.260 --> 00:08:09.080]   a platform for developers where you can actually use a combination\n",
            "[00:08:09.080 --> 00:08:10.560]   of various different kinds of models,\n",
            "[00:08:10.560 --> 00:08:13.120]   some which are from us, some which are open source,\n",
            "[00:08:13.120 --> 00:08:16.360]   some which may not be open source, and actually to actually pull together\n",
            "[00:08:16.360 --> 00:08:22.880]   and figure out how to deploy, you know, generative AI applications at scale\n",
            "[00:08:22.880 --> 00:08:26.320]   and understand and evaluate their performance in an efficient manner.\n",
            "[00:08:26.320 --> 00:08:29.080]   And that's something that we are planning to do at this --\n",
            "[00:08:29.080 --> 00:08:32.740]   and this platform is, you know, in the next couple of months,\n",
            "[00:08:32.740 --> 00:08:35.400]   we'll be coming out there, it will be available to developers.\n",
            "[00:08:35.400 --> 00:08:39.140]   But of course, those who want to start with the open source things\n",
            "[00:08:39.140 --> 00:08:42.680]   and hack for that, of course, please go ahead and do that as well.\n",
            "[00:08:42.680 --> 00:08:43.520]   >> That's phenomenal.\n",
            "[00:08:43.520 --> 00:08:48.300]   But how does it compare to OpenAI itself or Google?\n",
            "[00:08:48.300 --> 00:08:53.180]   >> See, at least the things that we are doing now, right?\n",
            "[00:08:53.180 --> 00:08:57.100]   I mean, one of the things that when we thought about building Sarban,\n",
            "[00:08:57.100 --> 00:09:01.360]   we said we want to build a full-stack generative AI company,\n",
            "[00:09:01.360 --> 00:09:05.080]   and different people have -- and our understanding of a stack is\n",
            "[00:09:05.080 --> 00:09:08.260]   that we need to know how to train models from scratch.\n",
            "[00:09:08.260 --> 00:09:12.380]   We need to know how to kind of figure out how to deploy models\n",
            "[00:09:12.380 --> 00:09:13.900]   to solve real-world use cases.\n",
            "[00:09:13.900 --> 00:09:17.260]   And we need to play in the ecosystem to make sure\n",
            "[00:09:17.260 --> 00:09:22.620]   that we can actually deploy population-scale applications, right?\n",
            "[00:09:22.620 --> 00:09:24.900]   So we were thinking about all of these things.\n",
            "[00:09:24.900 --> 00:09:29.440]   But still, the models we were talking about are, you know, fairly small models.\n",
            "[00:09:29.440 --> 00:09:32.340]   They are fairly small models, right, in the 7 to maybe\n",
            "[00:09:32.340 --> 00:09:34.880]   up to 70 billion kind of range we're talking about.\n",
            "[00:09:34.880 --> 00:09:39.940]   While these models like OpenAI and Google are obviously much bigger models, right?\n",
            "[00:09:39.940 --> 00:09:46.160]   But we want to understand the techniques and be able to build that muscle to do all\n",
            "[00:09:46.160 --> 00:09:49.940]   of these things, to make it available to people.\n",
            "[00:09:49.940 --> 00:09:53.100]   Now, those models are -- I mean, as I said, you know,\n",
            "[00:09:53.100 --> 00:09:56.220]   I think that there is space for all of those things.\n",
            "[00:09:56.480 --> 00:10:00.740]   And I think as even Sridhar was talking about earlier in the day,\n",
            "[00:10:00.740 --> 00:10:05.300]   we believe that these smaller models can do very --\n",
            "[00:10:05.300 --> 00:10:10.040]   I mean, many, many kind of domain-specific tasks extremely well,\n",
            "[00:10:10.040 --> 00:10:12.440]   probably even better than the larger models.\n",
            "[00:10:12.440 --> 00:10:14.660]   And that is really one of the key areas.\n",
            "[00:10:14.660 --> 00:10:19.920]   And so therefore, the value of these kinds of things, right, we're not aiming in these set\n",
            "[00:10:19.920 --> 00:10:22.280]   of models to build any AGI, right?\n",
            "[00:10:22.280 --> 00:10:23.480]   That's not our goal here.\n",
            "[00:10:23.740 --> 00:10:28.860]   Our goal is to make things that work extremely well for domain-specific use cases\n",
            "[00:10:28.860 --> 00:10:33.220]   or increase accessibility through language and all of those kinds of things.\n",
            "[00:10:33.220 --> 00:10:34.960]   >> And obviously, all of this unique to India.\n",
            "[00:10:34.960 --> 00:10:36.440]   But what is unique about India?\n",
            "[00:10:36.440 --> 00:10:41.000]   I mean, like, what is -- is there anything special in our ecosystem\n",
            "[00:10:41.000 --> 00:10:45.960]   that makes small models focused with Indian languages better for --\n",
            "[00:10:45.960 --> 00:10:47.020]   more suited for our problems?\n",
            "[00:10:47.020 --> 00:10:53.220]   >> So I think that, I mean, there are quite a few things that are unique about India, right?\n",
            "[00:10:53.660 --> 00:10:57.760]   The first thing is, I think, that we are a voice first nation.\n",
            "[00:10:57.760 --> 00:11:01.480]   So therefore, I think voice has to be the core to doing things.\n",
            "[00:11:01.480 --> 00:11:06.000]   The other thing, of course, India is extremely --\n",
            "[00:11:06.000 --> 00:11:09.700]   it's a cost-conscious country from a cost perspective.\n",
            "[00:11:09.700 --> 00:11:15.180]   Now, I would say that there are lots of interesting use cases where you can use open AI\n",
            "[00:11:15.180 --> 00:11:18.760]   and the cost structure works that when -- depending on your application.\n",
            "[00:11:18.760 --> 00:11:22.660]   But when you want to scale things to a massive level and make it work,\n",
            "[00:11:22.800 --> 00:11:25.140]   then you have to figure out how small models work.\n",
            "[00:11:25.140 --> 00:11:28.340]   So that's something that is also specific to India.\n",
            "[00:11:28.340 --> 00:11:32.820]   The third thing, which is specific to India, is really the success that India has had\n",
            "[00:11:32.820 --> 00:11:35.820]   in building all this digital public infrastructure.\n",
            "[00:11:35.820 --> 00:11:41.640]   When you add the AI layer on top of it, then you can actually get dramatic, you know,\n",
            "[00:11:41.640 --> 00:11:47.900]   dramatic I think multiplicative combinatorial effects based on doing things like that.\n",
            "[00:11:47.900 --> 00:11:48.900]   >> That's a phenomenal point.\n",
            "[00:11:48.900 --> 00:11:51.800]   Like, you know, it's like DPI to the power of AI almost in some ways.\n",
            "[00:11:52.220 --> 00:11:55.860]   And as a part of Adar, building Adar, no better person than you.\n",
            "[00:11:55.860 --> 00:12:01.780]   So in summary, what I'm hearing is small models specialize with trained\n",
            "[00:12:01.780 --> 00:12:05.740]   with Indic-specific language data suited for Indian problems\n",
            "[00:12:05.740 --> 00:12:08.900]   at a compelling cost point will be suited for us.\n",
            "[00:12:08.900 --> 00:12:12.760]   We're not solving some world autonomous vehicles or some complex problem.\n",
            "[00:12:12.760 --> 00:12:17.740]   We're solving some basic problems specifically focused on voice with multiple languages.\n",
            "[00:12:17.740 --> 00:12:19.420]   That is what you see as the future.\n",
            "[00:12:19.420 --> 00:12:21.080]   Am I paraphrasing this correctly?\n",
            "[00:12:21.340 --> 00:12:21.820]   >> No, yeah.\n",
            "[00:12:21.820 --> 00:12:26.740]   So I think that certainly, I mean, voice in Indian languages are an important part\n",
            "[00:12:26.740 --> 00:12:30.300]   of our strategy, but we will be building, you know, custom models\n",
            "[00:12:30.300 --> 00:12:33.220]   to solve various other kinds of problems as well, right?\n",
            "[00:12:33.220 --> 00:12:38.540]   It's not just limited to, I think, in different domains, working in different domains,\n",
            "[00:12:38.540 --> 00:12:43.460]   making building things based on unique data that enterprises have and things like that.\n",
            "[00:12:43.460 --> 00:12:44.960]   So that's something that we'll also look at.\n",
            "[00:12:44.960 --> 00:12:45.780]   >> Fair enough.\n",
            "[00:12:46.060 --> 00:12:51.140]   So coming back to the elephant in the room, no pun intended with open Hathi,\n",
            "[00:12:51.140 --> 00:12:53.880]   what about Babi Shacharwal and Kruthiram?\n",
            "[00:12:53.880 --> 00:12:55.380]   What is your take on that?\n",
            "[00:12:55.380 --> 00:12:56.400]   >> I think it's great.\n",
            "[00:12:56.400 --> 00:12:58.200]   I think it's wonderful, right?\n",
            "[00:12:58.200 --> 00:13:02.580]   I mean, the fact that the technology, AI, is so important\n",
            "[00:13:02.580 --> 00:13:05.540]   that we need multiple people working on it.\n",
            "[00:13:05.540 --> 00:13:09.100]   The fact that there are other people thinking is actually validates\n",
            "[00:13:09.100 --> 00:13:11.520]   that this is an important problem to be solved.\n",
            "[00:13:12.020 --> 00:13:17.380]   And I think that -- and we need everybody to come together and do that.\n",
            "[00:13:17.380 --> 00:13:18.980]   So I really welcome that.\n",
            "[00:13:18.980 --> 00:13:22.860]   I think it's great, and I think that there will be different people\n",
            "[00:13:22.860 --> 00:13:26.220]   who will have different takes as to how to solve this kind of problem.\n",
            "[00:13:26.220 --> 00:13:30.520]   And hopefully, as a result of that, the entire ecosystem benefits.\n",
            "[00:13:30.520 --> 00:13:34.180]   >> One more question, and then I want to talk about some\n",
            "[00:13:34.180 --> 00:13:36.220]   of the predictions that you've boldly made.\n",
            "[00:13:36.220 --> 00:13:39.240]   So Vivek, I usually ask people about what do you think the future will be,\n",
            "[00:13:39.240 --> 00:13:40.560]   and everybody usually hedges.\n",
            "[00:13:40.920 --> 00:13:44.780]   I asked Vivek, what do you think is going to happen by December 2024?\n",
            "[00:13:44.780 --> 00:13:47.920]   What do you think, sitting in this room, one year later, we can expect?\n",
            "[00:13:47.920 --> 00:13:49.940]   And he made three bold predictions.\n",
            "[00:13:49.940 --> 00:13:51.580]   So I want to talk about that.\n",
            "[00:13:51.580 --> 00:13:53.040]   Before that, I have one last question.\n",
            "[00:13:53.040 --> 00:13:57.260]   What are the top three applications that you think are relevant for India?\n",
            "[00:13:57.260 --> 00:13:59.580]   You heard Sridhar talk about medical.\n",
            "[00:13:59.580 --> 00:14:04.420]   Quick summary, what do you think the top three apps are for India, for AI?\n",
            "[00:14:05.060 --> 00:14:11.600]   >> So, I mean, I think that, as you said, things like education and medical are clearly areas\n",
            "[00:14:11.600 --> 00:14:15.060]   where I think that things can be leveraged.\n",
            "[00:14:15.060 --> 00:14:21.140]   The whole idea of all these kind of the DPI aspect of it is another major application\n",
            "[00:14:21.140 --> 00:14:24.540]   where things can happen, and here I'm talking about country-specific work.\n",
            "[00:14:24.540 --> 00:14:30.000]   And I think the whole idea, which Sridhar also talked about, was the concept of software, right?\n",
            "[00:14:30.000 --> 00:14:33.820]   And I think that, and clearly, we have a very large software industry,\n",
            "[00:14:33.820 --> 00:14:38.280]   and how to reimagine those things in this context is also something that's going to be.\n",
            "[00:14:38.280 --> 00:14:40.060]   >> Okay. Fair enough.\n",
            "[00:14:40.060 --> 00:14:43.780]   Are you guys ready for Vivek Raghavan's bold predictions?\n",
            "[00:14:43.780 --> 00:14:46.720]   Yes? No? I'm not hearing any yes, sir.\n",
            "[00:14:46.720 --> 00:14:47.680]   This is like a big deal.\n",
            "[00:14:47.680 --> 00:14:49.760]   He's like one of the smartest guys that I know.\n",
            "[00:14:49.760 --> 00:14:51.300]   He wants to make three predictions.\n",
            "[00:14:51.300 --> 00:14:53.220]   You don't want to hear it.\n",
            "[00:14:53.220 --> 00:14:54.220]   All right.\n",
            "[00:14:54.220 --> 00:14:59.140]   So I asked him, what do you think, you know, a year later, what do you think we can expect?\n",
            "[00:14:59.140 --> 00:15:02.980]   And he came up with three things, and usually people give very blah answers\n",
            "[00:15:02.980 --> 00:15:06.740]   when you ask a question like this, because they don't want to be caught wrong, not Vivek.\n",
            "[00:15:06.740 --> 00:15:07.620]   Vivek is bold.\n",
            "[00:15:07.620 --> 00:15:11.460]   So he basically said three things, and I'm going to list out the three things,\n",
            "[00:15:11.460 --> 00:15:13.100]   and then he's asked him about it.\n",
            "[00:15:13.100 --> 00:15:17.820]   So number one, he says, I would prefer to talk to an automated customer service\n",
            "[00:15:17.820 --> 00:15:21.340]   than a real person, because they'll give me a better answer.\n",
            "[00:15:21.340 --> 00:15:23.620]   So that is Vivek Raghavan's prediction, number one.\n",
            "[00:15:23.620 --> 00:15:29.100]   So number two is that when everybody is talking about a GPU shortage,\n",
            "[00:15:29.100 --> 00:15:32.620]   Vivek predicts that there'll be a GPU glut in India.\n",
            "[00:15:32.620 --> 00:15:34.540]   He thinks there'll be too much GPU, OK?\n",
            "[00:15:34.540 --> 00:15:37.580]   So if you want to short in media stock, this is a good time.\n",
            "[00:15:37.580 --> 00:15:44.260]   And number three, which was extremely unexpected, he said some companies will suddenly die.\n",
            "[00:15:44.260 --> 00:15:47.860]   OK, so Vivek, these are not what I expected.\n",
            "[00:15:47.860 --> 00:15:54.580]   So you want to quickly talk about each of them, why you just came up with these,\n",
            "[00:15:54.580 --> 00:15:56.740]   and then we'll throw the open for audience questions.\n",
            "[00:15:56.740 --> 00:16:01.500]   So I don't think I quite said it the way that Bala is kind of thinking.\n",
            "[00:16:01.500 --> 00:16:03.820]   But it's interesting.\n",
            "[00:16:03.820 --> 00:16:10.580]   But I think that the first thing that we said is I think that and I don't think that this is,\n",
            "[00:16:10.580 --> 00:16:17.700]   I think there will come a time when, you know, in areas of customer service, et cetera,\n",
            "[00:16:17.700 --> 00:16:20.580]   when you want to do something very specific.\n",
            "[00:16:20.580 --> 00:16:24.780]   Today, you know, when you call, when you call some kind of a bot, you actually end up,\n",
            "[00:16:24.780 --> 00:16:30.980]   you mostly try to disconnect the call or, you know, you're extremely upset that you're talking to a bot.\n",
            "[00:16:31.340 --> 00:16:35.900]   But I think that there will come a time, and I'm predicting it is sooner than later,\n",
            "[00:16:35.900 --> 00:16:41.820]   that you will actually get better responses from the bot than what the human representative,\n",
            "[00:16:41.820 --> 00:16:46.020]   that at least the average human representative that you could talk to could give.\n",
            "[00:16:46.020 --> 00:16:50.780]   And I think that that's just, I just said that there will come a time where, you know,\n",
            "[00:16:50.780 --> 00:16:58.020]   it's not a human you're talking to, but it's probably more likely to solve your intent than the human person.\n",
            "[00:16:58.020 --> 00:17:03.180]   That's just something that I think that could happen.\n",
            "[00:17:03.180 --> 00:17:06.380]   Definitely controversial, but we'll let it go.\n",
            "[00:17:06.380 --> 00:17:07.740]   What about the GPU glut?\n",
            "[00:17:07.740 --> 00:17:14.980]   No, no, yeah, so I don't think that, so I think that the fact that there is tremendous shortage right now,\n",
            "[00:17:14.980 --> 00:17:19.860]   I think that shortage will ease because that is how the cycles of things go, right?\n",
            "[00:17:19.860 --> 00:17:24.660]   When, you know, I think the fact that there was such a severe shortage last year,\n",
            "[00:17:25.140 --> 00:17:31.140]   you know, basically caused a number of different players to ramp up in various kinds of forms.\n",
            "[00:17:31.140 --> 00:17:34.460]   And I think that that will always go in a cycle.\n",
            "[00:17:34.460 --> 00:17:39.980]   But you may, we may find out that there are many, many more interesting problems that people will be able to solve.\n",
            "[00:17:39.980 --> 00:17:51.100]   I still remember, you know, we were at a Gen AI event in Bangalore and we were talking to people and we said,\n",
            "[00:17:51.100 --> 00:17:56.580]   you know, how many people have access to, you know, 4A100s, this was the question that I'd asked,\n",
            "[00:17:56.580 --> 00:18:01.500]   and nobody in the room, and these are all extremely enthusiastic Gen AI people, and nobody had access.\n",
            "[00:18:01.500 --> 00:18:03.500]   And I think that thing is going to change.\n",
            "[00:18:03.500 --> 00:18:08.940]   You will be able to get these kinds of things and people who want to hack and do things will have access to these things\n",
            "[00:18:08.940 --> 00:18:14.980]   at, without, you know, having to write a, you know, check, check.\n",
            "[00:18:14.980 --> 00:18:20.380]   Vivek is also a semiconductor guy before he went into Adar, so I would take his predictions very seriously.\n",
            "[00:18:20.380 --> 00:18:22.660]   So I don't know what, I'm going to sell my media stock.\n",
            "[00:18:22.660 --> 00:18:26.060]   I would not do that, but that's not what I said.\n",
            "[00:18:26.060 --> 00:18:29.260]   I want to blame you for this, if it goes up.\n",
            "[00:18:29.260 --> 00:18:32.620]   But the third one is pretty strange.\n",
            "[00:18:32.620 --> 00:18:37.780]   You know, companies are born, companies die, but you said some companies will suddenly die.\n",
            "[00:18:37.780 --> 00:18:38.620]   What does that mean?\n",
            "[00:18:38.620 --> 00:18:46.780]   No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI.\n",
            "[00:18:47.140 --> 00:18:54.500]   AI is a tool, right, and you have to use that, and you have to use that within your business process, right?\n",
            "[00:18:54.500 --> 00:19:01.940]   And how AI is being used, and so, and what's going to happen is that, I mean, I think this is true with, you know,\n",
            "[00:19:01.940 --> 00:19:11.380]   when someone said in terms of, you know, people, they said that the people who leverage AI will be, will be more effective than those who don't leverage AI.\n",
            "[00:19:11.700 --> 00:19:22.780]   And that was true for organizations also. Organizations that leverage AI in, fundamentally in their core business processes, will be more effective than those who don't, right?\n",
            "[00:19:22.780 --> 00:19:30.260]   And I think that's the thing, and you won't know the difference until one day it becomes too obvious, and it will be too late.\n",
            "[00:19:30.260 --> 00:19:39.260]   And I think that's the reason why everybody needs to think about what it means for your business, because you will, everything will be fine.\n",
            "[00:19:39.460 --> 00:19:51.100]   Then one day, somebody in your, either your competitor in your space or somebody brand new, coming into your space, will be reimagining your business process completely.\n",
            "[00:19:51.100 --> 00:19:58.820]   And at that stage, you will find that it's, you know, it's a very big, very tall, you know, mountain to climb.\n",
            "[00:19:58.820 --> 00:20:07.500]   And that's why I think it's important for both people and entities to think about how they will, you know, they will upgrade themselves,\n",
            "[00:20:07.500 --> 00:20:10.860]   or they will modify their business processes to, you know, to address it.\n",
            "[00:20:10.860 --> 00:20:17.140]   That's a very nuanced answer, and everybody, everybody here who's running a business should really think about it, because life will be the same.\n",
            "[00:20:17.140 --> 00:20:21.740]   And then suddenly, suddenly something will, you know, then there will be a step change.\n",
            "[00:20:21.740 --> 00:20:28.180]   I have a few more questions, but I'm sure the audience has a lot of questions for you. So how are we doing on time?\n",
            "[00:20:28.180 --> 00:20:35.900]   Okay. So does, okay, a lot of questions, so love to, is there a mic that we can pass around?\n",
            "[00:20:35.900 --> 00:20:46.900]   Thank you. My name is Karthik, I work for IT service industry.\n",
            "[00:20:46.900 --> 00:20:55.140]   So you're saying that you're working on LLM, sorry, it's fine-tuned LLM on top of LAMA.\n",
            "[00:20:55.140 --> 00:20:59.620]   My basic question, fundamental question is we don't have a fundamental foundational model for India.\n",
            "[00:20:59.620 --> 00:21:04.860]   Most of the models are basically using English or those kind of things.\n",
            "[00:21:04.860 --> 00:21:10.060]   For example, even Andrew was talking about the tokenizers and things like that.\n",
            "[00:21:10.060 --> 00:21:17.300]   So are you working on anything like that, or do you want to use mostly the existing models and run on top of it?\n",
            "[00:21:17.300 --> 00:21:21.220]   You asked a good question, you asked a cherry question for himself.\n",
            "[00:21:21.220 --> 00:21:28.060]   No, I think the interesting thing is that if you look at, and then we have actually a blog on this, on our website,\n",
            "[00:21:28.060 --> 00:21:38.460]   I think one of the things that we've built is we've actually built a customized tokenizer, which actually fundamentally changes the cost of some of these generations in Indian languages.\n",
            "[00:21:38.460 --> 00:21:44.860]   And I think that we're not just fine-tuning, we're actually, we are leveraging the existing pre-training,\n",
            "[00:21:44.860 --> 00:21:50.060]   but we are doing what's known as continual pre-training, which actually, but having said that, you know,\n",
            "[00:21:50.060 --> 00:21:56.460]   I think that once we have to figure out where is the data to train an extremely large model from scratch,\n",
            "[00:21:56.460 --> 00:22:00.460]   and some of those things are things which will happen over time.\n",
            "[00:22:00.460 --> 00:22:06.620]   But I think that, I think that, yes, I think that we will be doing various kinds of things.\n",
            "[00:22:06.620 --> 00:22:13.340]   But the interesting thing is that if I want to change the accessibility problem with an existing open source model,\n",
            "[00:22:13.340 --> 00:22:14.340]   how do I do that?\n",
            "[00:22:14.340 --> 00:22:20.620]   And that's the problem that we have, that we think we have solved, and it's going to be the heart of this open RT series.\n",
            "[00:22:20.620 --> 00:22:25.260]   It's extremely well explained in the blog, even I could understand it.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!./main -m models/ggml-small.en.bin -f '/content/Sarvam_output16000.wav'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niGvKUXAfSxz"
      },
      "source": [
        "Aligning transcript (text) - audio pairs using MFCC and DTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dookYtmTb9xv",
        "outputId": "e0efe4c4-79de-473b-def6-82f91839dc11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dtw-python\n",
            "  Downloading dtw_python-1.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (744 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m744.8/744.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from dtw-python) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from dtw-python) (1.11.4)\n",
            "Building wheels for collected packages: python_speech_features\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5870 sha256=503e13dd7313f471daf8f9777d8001bfef62cd022219e184d5b8eb61cbd72ec3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
            "Successfully built python_speech_features\n",
            "Installing collected packages: python_speech_features, dtw-python\n",
            "Successfully installed dtw-python-1.4.4 python_speech_features-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install python_speech_features dtw-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KdRNC2RcG5c",
        "outputId": "efe0be76-9298-4bdc-8ff5-cc9c408627b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mfcc_rate 100.0\n",
            "length of mfcc_features 157500\n",
            "Segment 1: >> Congratulations to you, Mr. Raghavan, for that.\n",
            "Start time: 0.0, End time: 2.12\n",
            "Frame range: 0 to 212\n",
            "length of segment_mfcc 212\n",
            "segment_mfcc.shape: (212, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 1: >> Congratulations to you, Mr. Raghavan, for that., Start time: 0.0, End time: 2.12\n",
            "Segment 2: Thank you so much for joining us.\n",
            "Start time: 2.12, End time: 3.4\n",
            "Frame range: 212 to 340\n",
            "length of segment_mfcc 128\n",
            "segment_mfcc.shape: (128, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 2: Thank you so much for joining us., Start time: 2.12, End time: 3.4\n",
            "Segment 3: Over to you.\n",
            "Start time: 3.4, End time: 4.12\n",
            "Frame range: 340 to 412\n",
            "length of segment_mfcc 72\n",
            "segment_mfcc.shape: (72, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 3: Over to you., Start time: 3.4, End time: 4.12\n",
            "Segment 4: >> Hi, everybody.\n",
            "Start time: 4.12, End time: 9.32\n",
            "Frame range: 412 to 932\n",
            "length of segment_mfcc 520\n",
            "segment_mfcc.shape: (520, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 4: >> Hi, everybody., Start time: 4.12, End time: 9.32\n",
            "Segment 5: How are you?\n",
            "Start time: 9.32, End time: 10.12\n",
            "Frame range: 932 to 1011\n",
            "length of segment_mfcc 79\n",
            "segment_mfcc.shape: (79, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 5: How are you?, Start time: 9.32, End time: 10.12\n",
            "Segment 6: Okay, I am not hearing this at all.\n",
            "Start time: 10.12, End time: 13.6\n",
            "Frame range: 1011 to 1360\n",
            "length of segment_mfcc 349\n",
            "segment_mfcc.shape: (349, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 6: Okay, I am not hearing this at all., Start time: 10.12, End time: 13.6\n",
            "Segment 7: It's like a post-lunch energy downer or something.\n",
            "Start time: 13.6, End time: 17.0\n",
            "Frame range: 1360 to 1700\n",
            "length of segment_mfcc 340\n",
            "segment_mfcc.shape: (340, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 7: It's like a post-lunch energy downer or something., Start time: 13.6, End time: 17.0\n",
            "Segment 8: Let's hear it.\n",
            "Start time: 17.0, End time: 18.78\n",
            "Frame range: 1700 to 1878\n",
            "length of segment_mfcc 178\n",
            "segment_mfcc.shape: (178, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 8: Let's hear it., Start time: 17.0, End time: 18.78\n",
            "Segment 9: Are you guys awake?\n",
            "Start time: 18.78, End time: 20.88\n",
            "Frame range: 1878 to 2088\n",
            "length of segment_mfcc 210\n",
            "segment_mfcc.shape: (210, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 9: Are you guys awake?, Start time: 18.78, End time: 20.88\n",
            "Segment 10: All right.\n",
            "Start time: 20.88, End time: 22.3\n",
            "Frame range: 2088 to 2230\n",
            "length of segment_mfcc 142\n",
            "segment_mfcc.shape: (142, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 10: All right., Start time: 20.88, End time: 22.3\n",
            "Segment 11: You better be, because we have a superstar guest here.\n",
            "Start time: 22.3, End time: 26.88\n",
            "Frame range: 2230 to 2688\n",
            "length of segment_mfcc 458\n",
            "segment_mfcc.shape: (458, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 11: You better be, because we have a superstar guest here., Start time: 22.3, End time: 26.88\n",
            "Segment 12: You heard the $41 million.\n",
            "Start time: 26.88, End time: 29.38\n",
            "Frame range: 2688 to 2938\n",
            "length of segment_mfcc 250\n",
            "segment_mfcc.shape: (250, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 12: You heard the $41 million., Start time: 26.88, End time: 29.38\n",
            "Segment 13: And I didn't hear, honestly, anything that she said after that.\n",
            "Start time: 29.38, End time: 33.68\n",
            "Frame range: 2938 to 3368\n",
            "length of segment_mfcc 430\n",
            "segment_mfcc.shape: (430, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 13: And I didn't hear, honestly, anything that she said after that., Start time: 29.38, End time: 33.68\n",
            "Segment 14: So we're going to ask for about $40 million from him\n",
            "Start time: 33.68, End time: 36.9\n",
            "Frame range: 3368 to 3690\n",
            "length of segment_mfcc 322\n",
            "segment_mfcc.shape: (322, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 14: So we're going to ask for about $40 million from him, Start time: 33.68, End time: 36.9\n",
            "Segment 15: by the end of this conversation.\n",
            "Start time: 36.9, End time: 38.08\n",
            "Frame range: 3690 to 3808\n",
            "length of segment_mfcc 118\n",
            "segment_mfcc.shape: (118, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 15: by the end of this conversation., Start time: 36.9, End time: 38.08\n",
            "Segment 16: Okay? But let's get started.\n",
            "Start time: 38.08, End time: 40.6\n",
            "Frame range: 3808 to 4060\n",
            "length of segment_mfcc 252\n",
            "segment_mfcc.shape: (252, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 16: Okay? But let's get started., Start time: 38.08, End time: 40.6\n",
            "Segment 17: I want to introduce Vivek and Pratyush,\n",
            "Start time: 40.6, End time: 43.72\n",
            "Frame range: 4060 to 4372\n",
            "length of segment_mfcc 312\n",
            "segment_mfcc.shape: (312, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 17: I want to introduce Vivek and Pratyush,, Start time: 40.6, End time: 43.72\n",
            "Segment 18: his co-founder who's not here.\n",
            "Start time: 43.72, End time: 44.94\n",
            "Frame range: 4372 to 4494\n",
            "length of segment_mfcc 122\n",
            "segment_mfcc.shape: (122, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 18: his co-founder who's not here., Start time: 43.72, End time: 44.94\n",
            "Segment 19: We wanted to start with playing a video of what OpenHati does.\n",
            "Start time: 44.94, End time: 51.0\n",
            "Frame range: 4494 to 5100\n",
            "length of segment_mfcc 606\n",
            "segment_mfcc.shape: (606, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 19: We wanted to start with playing a video of what OpenHati does., Start time: 44.94, End time: 51.0\n",
            "Segment 20: I encourage all of you to go to the website,\n",
            "Start time: 51.0, End time: 53.88\n",
            "Frame range: 5100 to 5388\n",
            "length of segment_mfcc 288\n",
            "segment_mfcc.shape: (288, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 20: I encourage all of you to go to the website,, Start time: 51.0, End time: 53.88\n",
            "Segment 21: sevaram.ai, and check it out.\n",
            "Start time: 53.88, End time: 55.34\n",
            "Frame range: 5388 to 5534\n",
            "length of segment_mfcc 146\n",
            "segment_mfcc.shape: (146, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 21: sevaram.ai, and check it out., Start time: 53.88, End time: 55.34\n",
            "Segment 22: But let me start by introducing Vivek.\n",
            "Start time: 55.34, End time: 58.18\n",
            "Frame range: 5534 to 5818\n",
            "length of segment_mfcc 284\n",
            "segment_mfcc.shape: (284, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 22: But let me start by introducing Vivek., Start time: 55.34, End time: 58.18\n",
            "Segment 23: Vivek is a dear friend, and he's very, very modest,\n",
            "Start time: 58.52, End time: 62.48\n",
            "Frame range: 5852 to 6248\n",
            "length of segment_mfcc 396\n",
            "segment_mfcc.shape: (396, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 23: Vivek is a dear friend, and he's very, very modest,, Start time: 58.52, End time: 62.48\n",
            "Segment 24: one of the most modest guys that I know.\n",
            "Start time: 62.48, End time: 64.32\n",
            "Frame range: 6248 to 6431\n",
            "length of segment_mfcc 183\n",
            "segment_mfcc.shape: (183, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 24: one of the most modest guys that I know., Start time: 62.48, End time: 64.32\n",
            "Segment 25: But his personal journey, Vivek, you've been --\n",
            "Start time: 64.32, End time: 66.78\n",
            "Frame range: 6431 to 6678\n",
            "length of segment_mfcc 247\n",
            "segment_mfcc.shape: (247, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 25: But his personal journey, Vivek, you've been --, Start time: 64.32, End time: 66.78\n",
            "Segment 26: you got a PhD from Carnegie Mellon.\n",
            "Start time: 66.78, End time: 69.14\n",
            "Frame range: 6678 to 6914\n",
            "length of segment_mfcc 236\n",
            "segment_mfcc.shape: (236, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 26: you got a PhD from Carnegie Mellon., Start time: 66.78, End time: 69.14\n",
            "Segment 27: You started and sold a company to Magma.\n",
            "Start time: 69.14, End time: 71.9\n",
            "Frame range: 6914 to 7190\n",
            "length of segment_mfcc 276\n",
            "segment_mfcc.shape: (276, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 27: You started and sold a company to Magma., Start time: 69.14, End time: 71.9\n",
            "Segment 28: And Vivek and I moved back to India from --\n",
            "Start time: 71.9, End time: 74.24\n",
            "Frame range: 7190 to 7423\n",
            "length of segment_mfcc 233\n",
            "segment_mfcc.shape: (233, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 28: And Vivek and I moved back to India from --, Start time: 71.9, End time: 74.24\n",
            "Segment 29: we were both in the valley on the same day, actually.\n",
            "Start time: 74.24, End time: 76.56\n",
            "Frame range: 7423 to 7656\n",
            "length of segment_mfcc 233\n",
            "segment_mfcc.shape: (233, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 29: we were both in the valley on the same day, actually., Start time: 74.24, End time: 76.56\n",
            "Segment 30: And you've been in India for the last 16 years.\n",
            "Start time: 76.56, End time: 79.5\n",
            "Frame range: 7656 to 7950\n",
            "length of segment_mfcc 294\n",
            "segment_mfcc.shape: (294, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 30: And you've been in India for the last 16 years., Start time: 76.56, End time: 79.5\n",
            "Segment 31: And what most people don't know is your journey at Adhar.\n",
            "Start time: 79.5, End time: 84.42\n",
            "Frame range: 7950 to 8442\n",
            "length of segment_mfcc 492\n",
            "segment_mfcc.shape: (492, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 31: And what most people don't know is your journey at Adhar., Start time: 79.5, End time: 84.42\n",
            "Segment 32: He spent 13 years selflessly at Adhar.\n",
            "Start time: 84.42, End time: 87.78\n",
            "Frame range: 8442 to 8778\n",
            "length of segment_mfcc 336\n",
            "segment_mfcc.shape: (336, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 32: He spent 13 years selflessly at Adhar., Start time: 84.42, End time: 87.78\n",
            "Segment 33: Nobody would have heard of him,\n",
            "Start time: 88.18, End time: 90.14\n",
            "Frame range: 8818 to 9014\n",
            "length of segment_mfcc 196\n",
            "segment_mfcc.shape: (196, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 33: Nobody would have heard of him,, Start time: 88.18, End time: 90.14\n",
            "Segment 34: but he was a pioneering technology visionary behind Adhar,\n",
            "Start time: 90.14, End time: 95.88\n",
            "Frame range: 9014 to 9588\n",
            "length of segment_mfcc 574\n",
            "segment_mfcc.shape: (574, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 34: but he was a pioneering technology visionary behind Adhar,, Start time: 90.14, End time: 95.88\n",
            "Segment 35: which we all take for granted today.\n",
            "Start time: 95.88, End time: 97.3\n",
            "Frame range: 9588 to 9730\n",
            "length of segment_mfcc 142\n",
            "segment_mfcc.shape: (142, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 35: which we all take for granted today., Start time: 95.88, End time: 97.3\n",
            "Segment 36: So please give it out.\n",
            "Start time: 97.3, End time: 100.1\n",
            "Frame range: 9730 to 10010\n",
            "length of segment_mfcc 280\n",
            "segment_mfcc.shape: (280, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 36: So please give it out., Start time: 97.3, End time: 100.1\n",
            "Segment 37: So, honestly, when people --\n",
            "Start time: 100.1, End time: 103.14\n",
            "Frame range: 10010 to 10314\n",
            "length of segment_mfcc 304\n",
            "segment_mfcc.shape: (304, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 37: So, honestly, when people --, Start time: 100.1, End time: 103.14\n",
            "Segment 38: when I think of selfless service,\n",
            "Start time: 103.14, End time: 104.46000000000001\n",
            "Frame range: 10314 to 10446\n",
            "length of segment_mfcc 132\n",
            "segment_mfcc.shape: (132, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 38: when I think of selfless service,, Start time: 103.14, End time: 104.46000000000001\n",
            "Segment 39: truly selfless service, I always think of Vivek.\n",
            "Start time: 104.46000000000001, End time: 107.94\n",
            "Frame range: 10446 to 10794\n",
            "length of segment_mfcc 348\n",
            "segment_mfcc.shape: (348, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 39: truly selfless service, I always think of Vivek., Start time: 104.46000000000001, End time: 107.94\n",
            "Segment 40: And since then, he also was at AI for Bharat,\n",
            "Start time: 107.94, End time: 110.75999999999999\n",
            "Frame range: 10794 to 11076\n",
            "length of segment_mfcc 282\n",
            "segment_mfcc.shape: (282, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 40: And since then, he also was at AI for Bharat,, Start time: 107.94, End time: 110.75999999999999\n",
            "Segment 41: which we're going to touch on,\n",
            "Start time: 110.75999999999999, End time: 112.34\n",
            "Frame range: 11076 to 11234\n",
            "length of segment_mfcc 158\n",
            "segment_mfcc.shape: (158, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 41: which we're going to touch on,, Start time: 110.75999999999999, End time: 112.34\n",
            "Segment 42: where he met Pratyush's other co-founder.\n",
            "Start time: 112.34, End time: 114.66\n",
            "Frame range: 11234 to 11466\n",
            "length of segment_mfcc 232\n",
            "segment_mfcc.shape: (232, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 42: where he met Pratyush's other co-founder., Start time: 112.34, End time: 114.66\n",
            "Segment 43: Pratyush had a PhD from ETH at Zurich.\n",
            "Start time: 114.66, End time: 117.88\n",
            "Frame range: 11466 to 11788\n",
            "length of segment_mfcc 322\n",
            "segment_mfcc.shape: (322, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 43: Pratyush had a PhD from ETH at Zurich., Start time: 114.66, End time: 117.88\n",
            "Segment 44: He was in the IBM research.\n",
            "Start time: 118.24000000000001, End time: 120.42\n",
            "Frame range: 11824 to 12042\n",
            "length of segment_mfcc 218\n",
            "segment_mfcc.shape: (218, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 44: He was in the IBM research., Start time: 118.24000000000001, End time: 120.42\n",
            "Segment 45: He was at Microsoft Research, playing a key role\n",
            "Start time: 120.42, End time: 122.52\n",
            "Frame range: 12042 to 12252\n",
            "length of segment_mfcc 210\n",
            "segment_mfcc.shape: (210, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 45: He was at Microsoft Research, playing a key role, Start time: 120.42, End time: 122.52\n",
            "Segment 46: in the faculty at IIT Madras and at AI for Bharat.\n",
            "Start time: 122.52, End time: 126.04\n",
            "Frame range: 12252 to 12604\n",
            "length of segment_mfcc 352\n",
            "segment_mfcc.shape: (352, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 46: in the faculty at IIT Madras and at AI for Bharat., Start time: 122.52, End time: 126.04\n",
            "Segment 47: So that's a little brief introduction about them.\n",
            "Start time: 126.04, End time: 128.76\n",
            "Frame range: 12604 to 12876\n",
            "length of segment_mfcc 272\n",
            "segment_mfcc.shape: (272, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 47: So that's a little brief introduction about them., Start time: 126.04, End time: 128.76\n",
            "Segment 48: These guys are modest, modest engineers.\n",
            "Start time: 128.76, End time: 130.92\n",
            "Frame range: 12876 to 13091\n",
            "length of segment_mfcc 215\n",
            "segment_mfcc.shape: (215, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 48: These guys are modest, modest engineers., Start time: 128.76, End time: 130.92\n",
            "Segment 49: So they don't toot their own horn.\n",
            "Start time: 130.92, End time: 133.32\n",
            "Frame range: 13091 to 13332\n",
            "length of segment_mfcc 241\n",
            "segment_mfcc.shape: (241, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 49: So they don't toot their own horn., Start time: 130.92, End time: 133.32\n",
            "Segment 50: So forgive me for tooting their horn in this case.\n",
            "Start time: 133.32, End time: 137.04\n",
            "Frame range: 13332 to 13704\n",
            "length of segment_mfcc 372\n",
            "segment_mfcc.shape: (372, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 50: So forgive me for tooting their horn in this case., Start time: 133.32, End time: 137.04\n",
            "Segment 51: But let's jump right in about the money, funding.\n",
            "Start time: 137.04, End time: 142.04\n",
            "Frame range: 13704 to 14204\n",
            "length of segment_mfcc 500\n",
            "segment_mfcc.shape: (500, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 51: But let's jump right in about the money, funding., Start time: 137.04, End time: 142.04\n",
            "Segment 52: Forty-one million bucks, man.\n",
            "Start time: 142.04, End time: 143.72\n",
            "Frame range: 14204 to 14372\n",
            "length of segment_mfcc 168\n",
            "segment_mfcc.shape: (168, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 52: Forty-one million bucks, man., Start time: 142.04, End time: 143.72\n",
            "Segment 53: That's a lot of money, right?\n",
            "Start time: 143.72, End time: 145.7\n",
            "Frame range: 14372 to 14569\n",
            "length of segment_mfcc 197\n",
            "segment_mfcc.shape: (197, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 53: That's a lot of money, right?, Start time: 143.72, End time: 145.7\n",
            "Segment 54: Every entrepreneur here is saying,\n",
            "Start time: 145.7, End time: 147.04\n",
            "Frame range: 14569 to 14704\n",
            "length of segment_mfcc 135\n",
            "segment_mfcc.shape: (135, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 54: Every entrepreneur here is saying,, Start time: 145.7, End time: 147.04\n",
            "Segment 55: \"What the hell did these guys do?\"\n",
            "Start time: 147.04, End time: 148.57999999999998\n",
            "Frame range: 14704 to 14857\n",
            "length of segment_mfcc 153\n",
            "segment_mfcc.shape: (153, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 55: \"What the hell did these guys do?\", Start time: 147.04, End time: 148.57999999999998\n",
            "Segment 56: What did the investors see to write such a big check?\n",
            "Start time: 148.57999999999998, End time: 151.56\n",
            "Frame range: 14857 to 15156\n",
            "length of segment_mfcc 299\n",
            "segment_mfcc.shape: (299, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 56: What did the investors see to write such a big check?, Start time: 148.57999999999998, End time: 151.56\n",
            "Segment 57: >> No, I think it's a new trend of what's going on in India.\n",
            "Start time: 151.56, End time: 158.0\n",
            "Frame range: 15156 to 15800\n",
            "length of segment_mfcc 644\n",
            "segment_mfcc.shape: (644, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 57: >> No, I think it's a new trend of what's going on in India., Start time: 151.56, End time: 158.0\n",
            "Segment 58: I think that for the very first time,\n",
            "Start time: 158.0, End time: 159.92000000000002\n",
            "Frame range: 15800 to 15992\n",
            "length of segment_mfcc 192\n",
            "segment_mfcc.shape: (192, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 58: I think that for the very first time,, Start time: 158.0, End time: 159.92000000000002\n",
            "Segment 59: I think the investors have looked at, you know,\n",
            "Start time: 159.92000000000002, End time: 163.1\n",
            "Frame range: 15992 to 16310\n",
            "length of segment_mfcc 318\n",
            "segment_mfcc.shape: (318, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 59: I think the investors have looked at, you know,, Start time: 159.92000000000002, End time: 163.1\n",
            "Segment 60: let's try and build something deep tech out of the country.\n",
            "Start time: 163.1, End time: 166.02\n",
            "Frame range: 16310 to 16602\n",
            "length of segment_mfcc 292\n",
            "segment_mfcc.shape: (292, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 60: let's try and build something deep tech out of the country., Start time: 163.1, End time: 166.02\n",
            "Segment 61: And let's try to figure out how to build something\n",
            "Start time: 166.02, End time: 168.88\n",
            "Frame range: 16602 to 16888\n",
            "length of segment_mfcc 286\n",
            "segment_mfcc.shape: (286, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 61: And let's try to figure out how to build something, Start time: 166.02, End time: 168.88\n",
            "Segment 62: as a foundational technology out of the country.\n",
            "Start time: 168.88, End time: 170.92000000000002\n",
            "Frame range: 16888 to 17092\n",
            "length of segment_mfcc 204\n",
            "segment_mfcc.shape: (204, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 62: as a foundational technology out of the country., Start time: 168.88, End time: 170.92000000000002\n",
            "Segment 63: And that's really what's really exciting, you know?\n",
            "Start time: 170.92000000000002, End time: 173.54\n",
            "Frame range: 17092 to 17354\n",
            "length of segment_mfcc 262\n",
            "segment_mfcc.shape: (262, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 63: And that's really what's really exciting, you know?, Start time: 170.92000000000002, End time: 173.54\n",
            "Segment 64: And I think that about, you know, as Bala was mentioning\n",
            "Start time: 173.54, End time: 180.28\n",
            "Frame range: 17354 to 18028\n",
            "length of segment_mfcc 674\n",
            "segment_mfcc.shape: (674, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 64: And I think that about, you know, as Bala was mentioning, Start time: 173.54, End time: 180.28\n",
            "Segment 65: for the past 15 years, I've been kind of working in kind of,\n",
            "Start time: 180.28, End time: 183.88\n",
            "Frame range: 18028 to 18388\n",
            "length of segment_mfcc 360\n",
            "segment_mfcc.shape: (360, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 65: for the past 15 years, I've been kind of working in kind of,, Start time: 180.28, End time: 183.88\n",
            "Segment 66: you know, both digital public infrastructure and kind\n",
            "Start time: 183.88, End time: 188.42\n",
            "Frame range: 18388 to 18842\n",
            "length of segment_mfcc 454\n",
            "segment_mfcc.shape: (454, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 66: you know, both digital public infrastructure and kind, Start time: 183.88, End time: 188.42\n",
            "Segment 67: of nonprofit kind of things.\n",
            "Start time: 188.42, End time: 190.56\n",
            "Frame range: 18842 to 19056\n",
            "length of segment_mfcc 214\n",
            "segment_mfcc.shape: (214, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 67: of nonprofit kind of things., Start time: 188.42, End time: 190.56\n",
            "Segment 68: And when this whole thing of generative AI came about,\n",
            "Start time: 190.56, End time: 193.7\n",
            "Frame range: 19056 to 19370\n",
            "length of segment_mfcc 314\n",
            "segment_mfcc.shape: (314, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 68: And when this whole thing of generative AI came about,, Start time: 190.56, End time: 193.7\n",
            "Segment 69: I said, you know, we said, \"Okay,\n",
            "Start time: 193.7, End time: 196.07999999999998\n",
            "Frame range: 19370 to 19608\n",
            "length of segment_mfcc 238\n",
            "segment_mfcc.shape: (238, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 69: I said, you know, we said, \"Okay,, Start time: 193.7, End time: 196.07999999999998\n",
            "Segment 70: how can I actually make a difference in this space?\"\n",
            "Start time: 196.07999999999998, End time: 199.16\n",
            "Frame range: 19608 to 19916\n",
            "length of segment_mfcc 308\n",
            "segment_mfcc.shape: (308, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 70: how can I actually make a difference in this space?\", Start time: 196.07999999999998, End time: 199.16\n",
            "Segment 71: And I said, \"Maybe this is the opportunity to actually come\n",
            "Start time: 199.16, End time: 202.44\n",
            "Frame range: 19916 to 20244\n",
            "length of segment_mfcc 328\n",
            "segment_mfcc.shape: (328, 13)\n",
            "mfcc_features.shape: (157500, 13)\n",
            "Processed segment 71: And I said, \"Maybe this is the opportunity to actually come, Start time: 199.16, End time: 202.44\n",
            "Segment 72: out and really build something, you know, and the only way\n",
            "Start time: 202.44, End time: 207.18\n",
            "Frame range: 20244 to 20718\n",
            "length of segment_mfcc 474\n",
            "segment_mfcc.shape: (474, 13)\n",
            "mfcc_features.shape: (157500, 13)\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import re\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from python_speech_features import mfcc\n",
        "from dtw import dtw\n",
        "\n",
        "def parse_output(output):\n",
        "    lines = output.split('\\n')\n",
        "    timestamps = []\n",
        "    transcripts = []\n",
        "\n",
        "    # for line in output.split('\\n'):\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        match = re.match(r'\\[(.*?)\\]   (.*)', line)\n",
        "        if match:\n",
        "            timestamps.append(match.group(1))\n",
        "            transcripts.append(match.group(2))\n",
        "\n",
        "        print(timestamps)\n",
        "        print(transcripts)\n",
        "    return timestamps, transcripts\n",
        "\n",
        "def timestamp_to_seconds(timestamp):\n",
        "    parts = timestamp.strip(\"[]\").split(':')\n",
        "    hours = int(parts[0])\n",
        "    minutes = int(parts[1])\n",
        "    seconds = float(parts[2])\n",
        "    return hours * 3600 + minutes * 60 + seconds\n",
        "\n",
        "def align_transcription_with_audio(audio_file, timestamps, transcripts):\n",
        "    audio = AudioSegment.from_wav(audio_file)\n",
        "    rate = audio.frame_rate\n",
        "    # print(f\"rate {rate}\")\n",
        "    audio_data = np.array(audio.get_array_of_samples())\n",
        "    # print(f\"audio_data {audio_data}\")\n",
        "    mfcc_features = mfcc(audio_data, rate)\n",
        "    # print(f\"mfcc_features {mfcc_features}\")\n",
        "\n",
        "    # Calculate the MFCC frame rate\n",
        "    hop_length = 160  # Default hop length for MFCC in python_speech_features\n",
        "    mfcc_rate = rate / hop_length\n",
        "\n",
        "    # mfcc_rate = len(mfcc_features) / (len(audio_data) / rate)  # MFCC features per second\n",
        "    print(f\"mfcc_rate {mfcc_rate}\")\n",
        "    print(f\"length of mfcc_features {len(mfcc_features)}\")\n",
        "\n",
        "    alignment = []\n",
        "    alignment_path = []\n",
        "\n",
        "    # for start, end, text in zip(timestamps, transcripts):\n",
        "    for i, (timestamp, text) in enumerate(zip(timestamps, transcripts)):\n",
        "        start, end = timestamp.split(\" --> \")\n",
        "        start_time = timestamp_to_seconds(start)\n",
        "        end_time = timestamp_to_seconds(end)\n",
        "\n",
        "        print(f\"Segment {i+1}: {text}\")\n",
        "        print(f\"Start time: {start_time}, End time: {end_time}\")\n",
        "        print(f\"Frame range: {int(start_time * mfcc_rate)} to {int(end_time * mfcc_rate)}\")\n",
        "\n",
        "        start_index = int(start_time * mfcc_rate)\n",
        "        end_index = int(end_time * mfcc_rate)\n",
        "\n",
        "        if start_index < 0 or end_index > len(mfcc_features):\n",
        "            print(f\"Skipping out of bounds segment MFCC for segment {i+1}\")\n",
        "            continue\n",
        "\n",
        "        segment_mfcc = mfcc_features[start_index:end_index]\n",
        "        print(f\"length of segment_mfcc {len(segment_mfcc)}\")\n",
        "\n",
        "        # print(f\"segment_mfcc {segment_mfcc}\")\n",
        "        # print(f\"segment_mfcc.T {segment_mfcc.T}\")\n",
        "\n",
        "        if segment_mfcc.T.shape[1] == 0:\n",
        "           # Skip this segment as it does not contain enough information for MFCC extraction\n",
        "           print(f\"Skipping empty segment MFCC for segment {i+1}\")\n",
        "           continue\n",
        "\n",
        "        # print(segment_mfcc.T.shape)\n",
        "        # print(mfcc_features.T.shape)\n",
        "        print(f\"segment_mfcc.shape: {segment_mfcc.shape}\")\n",
        "        print(f\"mfcc_features.shape: {mfcc_features.shape}\")\n",
        "\n",
        "        # dtw_result = dtw(segment_mfcc.T, mfcc_features.T)\n",
        "        dtw_result = dtw(segment_mfcc, mfcc_features)\n",
        "        # print(type(dtw_result))\n",
        "        # print(dir(dtw_result))\n",
        "\n",
        "        dist = dtw_result.distance\n",
        "        path = dtw_result.index1s\n",
        "\n",
        "        alignment.append((start_time, end_time, text))\n",
        "        alignment_path.append(path)\n",
        "\n",
        "        print(f\"Processed segment {i+1}: {text}, Start time: {start_time}, End time: {end_time}\")\n",
        "\n",
        "    return alignment, alignment_path\n",
        "\n",
        "output = \"\"\"\n",
        "[00:00:00.000 --> 00:00:02.120]   >> Congratulations to you, Mr. Raghavan, for that.\n",
        "[00:00:02.120 --> 00:00:03.400]   Thank you so much for joining us.\n",
        "[00:00:03.400 --> 00:00:04.120]   Over to you.\n",
        "[00:00:04.120 --> 00:00:09.320]   >> Hi, everybody.\n",
        "[00:00:09.320 --> 00:00:10.120]   How are you?\n",
        "[00:00:10.120 --> 00:00:13.600]   Okay, I am not hearing this at all.\n",
        "[00:00:13.600 --> 00:00:17.000]   It's like a post-lunch energy downer or something.\n",
        "[00:00:17.000 --> 00:00:18.780]   Let's hear it.\n",
        "[00:00:18.780 --> 00:00:20.880]   Are you guys awake?\n",
        "[00:00:20.880 --> 00:00:22.300]   All right.\n",
        "[00:00:22.300 --> 00:00:26.880]   You better be, because we have a superstar guest here.\n",
        "[00:00:26.880 --> 00:00:29.380]   You heard the $41 million.\n",
        "[00:00:29.380 --> 00:00:33.680]   And I didn't hear, honestly, anything that she said after that.\n",
        "[00:00:33.680 --> 00:00:36.900]   So we're going to ask for about $40 million from him\n",
        "[00:00:36.900 --> 00:00:38.080]   by the end of this conversation.\n",
        "[00:00:38.080 --> 00:00:40.600]   Okay? But let's get started.\n",
        "[00:00:40.600 --> 00:00:43.720]   I want to introduce Vivek and Pratyush,\n",
        "[00:00:43.720 --> 00:00:44.940]   his co-founder who's not here.\n",
        "[00:00:44.940 --> 00:00:51.000]   We wanted to start with playing a video of what OpenHati does.\n",
        "[00:00:51.000 --> 00:00:53.880]   I encourage all of you to go to the website,\n",
        "[00:00:53.880 --> 00:00:55.340]   sevaram.ai, and check it out.\n",
        "[00:00:55.340 --> 00:00:58.180]   But let me start by introducing Vivek.\n",
        "[00:00:58.520 --> 00:01:02.480]   Vivek is a dear friend, and he's very, very modest,\n",
        "[00:01:02.480 --> 00:01:04.320]   one of the most modest guys that I know.\n",
        "[00:01:04.320 --> 00:01:06.780]   But his personal journey, Vivek, you've been --\n",
        "[00:01:06.780 --> 00:01:09.140]   you got a PhD from Carnegie Mellon.\n",
        "[00:01:09.140 --> 00:01:11.900]   You started and sold a company to Magma.\n",
        "[00:01:11.900 --> 00:01:14.240]   And Vivek and I moved back to India from --\n",
        "[00:01:14.240 --> 00:01:16.560]   we were both in the valley on the same day, actually.\n",
        "[00:01:16.560 --> 00:01:19.500]   And you've been in India for the last 16 years.\n",
        "[00:01:19.500 --> 00:01:24.420]   And what most people don't know is your journey at Adhar.\n",
        "[00:01:24.420 --> 00:01:27.780]   He spent 13 years selflessly at Adhar.\n",
        "[00:01:28.180 --> 00:01:30.140]   Nobody would have heard of him,\n",
        "[00:01:30.140 --> 00:01:35.880]   but he was a pioneering technology visionary behind Adhar,\n",
        "[00:01:35.880 --> 00:01:37.300]   which we all take for granted today.\n",
        "[00:01:37.300 --> 00:01:40.100]   So please give it out.\n",
        "[00:01:40.100 --> 00:01:43.140]   So, honestly, when people --\n",
        "[00:01:43.140 --> 00:01:44.460]   when I think of selfless service,\n",
        "[00:01:44.460 --> 00:01:47.940]   truly selfless service, I always think of Vivek.\n",
        "[00:01:47.940 --> 00:01:50.760]   And since then, he also was at AI for Bharat,\n",
        "[00:01:50.760 --> 00:01:52.340]   which we're going to touch on,\n",
        "[00:01:52.340 --> 00:01:54.660]   where he met Pratyush's other co-founder.\n",
        "[00:01:54.660 --> 00:01:57.880]   Pratyush had a PhD from ETH at Zurich.\n",
        "[00:01:58.240 --> 00:02:00.420]   He was in the IBM research.\n",
        "[00:02:00.420 --> 00:02:02.520]   He was at Microsoft Research, playing a key role\n",
        "[00:02:02.520 --> 00:02:06.040]   in the faculty at IIT Madras and at AI for Bharat.\n",
        "[00:02:06.040 --> 00:02:08.760]   So that's a little brief introduction about them.\n",
        "[00:02:08.760 --> 00:02:10.920]   These guys are modest, modest engineers.\n",
        "[00:02:10.920 --> 00:02:13.320]   So they don't toot their own horn.\n",
        "[00:02:13.320 --> 00:02:17.040]   So forgive me for tooting their horn in this case.\n",
        "[00:02:17.040 --> 00:02:22.040]   But let's jump right in about the money, funding.\n",
        "[00:02:22.040 --> 00:02:23.720]   Forty-one million bucks, man.\n",
        "[00:02:23.720 --> 00:02:25.700]   That's a lot of money, right?\n",
        "[00:02:25.700 --> 00:02:27.040]   Every entrepreneur here is saying,\n",
        "[00:02:27.040 --> 00:02:28.580]   \"What the hell did these guys do?\"\n",
        "[00:02:28.580 --> 00:02:31.560]   What did the investors see to write such a big check?\n",
        "[00:02:31.560 --> 00:02:38.000]   >> No, I think it's a new trend of what's going on in India.\n",
        "[00:02:38.000 --> 00:02:39.920]   I think that for the very first time,\n",
        "[00:02:39.920 --> 00:02:43.100]   I think the investors have looked at, you know,\n",
        "[00:02:43.100 --> 00:02:46.020]   let's try and build something deep tech out of the country.\n",
        "[00:02:46.020 --> 00:02:48.880]   And let's try to figure out how to build something\n",
        "[00:02:48.880 --> 00:02:50.920]   as a foundational technology out of the country.\n",
        "[00:02:50.920 --> 00:02:53.540]   And that's really what's really exciting, you know?\n",
        "[00:02:53.540 --> 00:03:00.280]   And I think that about, you know, as Bala was mentioning\n",
        "[00:03:00.280 --> 00:03:03.880]   for the past 15 years, I've been kind of working in kind of,\n",
        "[00:03:03.880 --> 00:03:08.420]   you know, both digital public infrastructure and kind\n",
        "[00:03:08.420 --> 00:03:10.560]   of nonprofit kind of things.\n",
        "[00:03:10.560 --> 00:03:13.700]   And when this whole thing of generative AI came about,\n",
        "[00:03:13.700 --> 00:03:16.080]   I said, you know, we said, \"Okay,\n",
        "[00:03:16.080 --> 00:03:19.160]   how can I actually make a difference in this space?\"\n",
        "[00:03:19.160 --> 00:03:22.440]   And I said, \"Maybe this is the opportunity to actually come\n",
        "[00:03:22.440 --> 00:03:27.180]   out and really build something, you know, and the only way\n",
        "[00:03:27.180 --> 00:03:29.120]   that we realize that you can do it is actually\n",
        "[00:03:29.120 --> 00:03:32.700]   in the private sector.\"\n",
        "[00:03:32.700 --> 00:03:35.520]   And I think that's -- and then we went out there and we said,\n",
        "[00:03:35.520 --> 00:03:38.180]   \"We want to build something,\" which is a continuation, right?\n",
        "[00:03:38.180 --> 00:03:41.300]   I mean, and fundamentally, the question is the reason\n",
        "[00:03:41.300 --> 00:03:43.800]   of what we want to do at server AI is we want\n",
        "[00:03:43.800 --> 00:03:48.340]   to basically make generative AI available and accessible\n",
        "[00:03:48.340 --> 00:03:49.700]   to the people in the country.\n",
        "[00:03:49.700 --> 00:03:51.360]   And that's the intent.\n",
        "[00:03:51.360 --> 00:03:53.440]   And when we said that we want to do this,\n",
        "[00:03:53.440 --> 00:03:56.280]   there was a resonance in the investment community.\n",
        "[00:03:56.280 --> 00:03:59.940]   And I think it's a responsibility to really to show\n",
        "[00:03:59.940 --> 00:04:02.740]   that something like this can be built out of India.\n",
        "[00:04:02.740 --> 00:04:06.680]   So we see that as confidence and a responsibility.\n",
        "[00:04:06.680 --> 00:04:09.640]   And I also hope it's a trend that, you know,\n",
        "[00:04:09.640 --> 00:04:12.360]   that there are many more people like us who are backed.\n",
        "[00:04:12.360 --> 00:04:16.680]   Because if you look at it, maybe it's a large number in a,\n",
        "[00:04:16.680 --> 00:04:19.660]   you know, in the Indian context, but in the global context,\n",
        "[00:04:19.940 --> 00:04:21.460]   I think there is just -- there should be many,\n",
        "[00:04:21.460 --> 00:04:23.440]   many more entrepreneurs who are backed\n",
        "[00:04:23.440 --> 00:04:24.740]   to do things in India.\n",
        "[00:04:24.740 --> 00:04:26.560]   >> Yeah. I'm going to come back\n",
        "[00:04:26.560 --> 00:04:27.580]   to the many more entrepreneurs.\n",
        "[00:04:27.580 --> 00:04:30.920]   I'm obviously going to ask you about Babish's krutram.\n",
        "[00:04:30.920 --> 00:04:33.560]   So we're going to come back to that question.\n",
        "[00:04:33.560 --> 00:04:36.500]   But again, $41 million.\n",
        "[00:04:36.500 --> 00:04:39.840]   I mean, all of what you said, you know, $2 million, you know,\n",
        "[00:04:39.840 --> 00:04:43.080]   that's a good amount of money for a startup, which, you know,\n",
        "[00:04:43.080 --> 00:04:44.480]   which has not yet built anything.\n",
        "[00:04:44.480 --> 00:04:46.340]   What are you going to do with all this money?\n",
        "[00:04:47.420 --> 00:04:50.240]   >> I can solve the problem.\n",
        "[00:04:50.240 --> 00:04:51.980]   I can have a perfect solution for the problem.\n",
        "[00:04:51.980 --> 00:04:54.360]   >> I think in the last week, I've got lots of calls\n",
        "[00:04:54.360 --> 00:04:57.960]   of lots of people telling me how I can -- no, but --\n",
        "[00:04:57.960 --> 00:04:59.260]   >> I know you first, okay?\n",
        "[00:04:59.260 --> 00:05:00.960]   I'll be landed in the country the same day.\n",
        "[00:05:00.960 --> 00:05:01.880]   I'm in the front of the queue.\n",
        "[00:05:01.880 --> 00:05:07.820]   >> No, but honestly, I think the key thing in this is\n",
        "[00:05:07.820 --> 00:05:09.420]   to putting together an amazing team.\n",
        "[00:05:09.420 --> 00:05:12.120]   And we actually have an amazing team, but we believe\n",
        "[00:05:12.120 --> 00:05:15.120]   that it is talent that will drive this kind of thing.\n",
        "[00:05:15.120 --> 00:05:18.000]   And so it is to get key talent.\n",
        "[00:05:18.000 --> 00:05:19.680]   And of course, the other thing is compute.\n",
        "[00:05:19.680 --> 00:05:23.160]   This is extremely expensive compute-wise\n",
        "[00:05:23.160 --> 00:05:25.200]   to actually do these kinds of things.\n",
        "[00:05:25.200 --> 00:05:27.900]   And I think that those are the two primary things\n",
        "[00:05:27.900 --> 00:05:30.200]   that, you know, we'd use this for.\n",
        "[00:05:30.200 --> 00:05:34.880]   >> Okay. I'm computing in my own head as an entrepreneur.\n",
        "[00:05:34.880 --> 00:05:37.260]   Talent, okay, you have like 20, 15 people.\n",
        "[00:05:37.260 --> 00:05:38.840]   How much are you paying these guys?\n",
        "[00:05:38.840 --> 00:05:41.520]   But okay, well, we won't touch on that.\n",
        "[00:05:41.520 --> 00:05:43.680]   But let's talk about what you guys actually built.\n",
        "[00:05:43.680 --> 00:05:46.360]   What does OpenHati -- how would you explain OpenHati\n",
        "[00:05:46.360 --> 00:05:48.640]   to many people here who might not have known about it?\n",
        "[00:05:48.640 --> 00:05:53.660]   >> So I think OpenHati is -- so first of all, right, we come from --\n",
        "[00:05:53.660 --> 00:05:57.160]   I personally come from the open source ecosystem.\n",
        "[00:05:57.160 --> 00:06:00.040]   And we -- and also from the DPI ecosystem.\n",
        "[00:06:00.040 --> 00:06:02.560]   So we believe that for this to work,\n",
        "[00:06:02.560 --> 00:06:05.280]   we need the ecosystem to be successful.\n",
        "[00:06:05.280 --> 00:06:09.000]   And as a result of that, one of the first things we did was, hey,\n",
        "[00:06:09.000 --> 00:06:11.840]   there are these open source large language models\n",
        "[00:06:11.840 --> 00:06:12.680]   that exist, right?\n",
        "[00:06:12.680 --> 00:06:15.840]   I mean, everybody knows about the Lama family from META.\n",
        "[00:06:15.840 --> 00:06:18.480]   They also -- there are others like Mistral.\n",
        "[00:06:18.480 --> 00:06:21.920]   There are a bunch of open source, you know,\n",
        "[00:06:21.920 --> 00:06:23.160]   large language models.\n",
        "[00:06:23.160 --> 00:06:25.920]   And then we said, is there any way\n",
        "[00:06:25.920 --> 00:06:28.900]   that take an existing open source model\n",
        "[00:06:28.900 --> 00:06:30.760]   and teach it language skills, right?\n",
        "[00:06:30.760 --> 00:06:34.560]   I mean, and that is really the, you know, what we decide --\n",
        "[00:06:34.560 --> 00:06:37.060]   what we said that can we do something like that?\n",
        "[00:06:37.060 --> 00:06:41.520]   And is this a, you know, relatively frugal way of actually,\n",
        "[00:06:41.520 --> 00:06:44.960]   you know, making models, you know,\n",
        "[00:06:44.960 --> 00:06:47.520]   work in diverse languages?\n",
        "[00:06:47.520 --> 00:06:49.360]   Because the truth is still today.\n",
        "[00:06:49.360 --> 00:06:52.320]   I mean, if you look at the amount of data\n",
        "[00:06:52.320 --> 00:06:54.900]   and knowledge, it is still -- English dominates these things.\n",
        "[00:06:54.900 --> 00:06:57.560]   And I think that how do you actually take\n",
        "[00:06:57.560 --> 00:06:59.820]   and make it understand Indian language,\n",
        "[00:06:59.820 --> 00:07:02.880]   understand Indian context, and all of those things in --\n",
        "[00:07:02.880 --> 00:07:05.040]   actually in an efficient way?\n",
        "[00:07:05.040 --> 00:07:07.100]   And therefore, this was an attempt to do that.\n",
        "[00:07:07.480 --> 00:07:11.800]   And it's -- OpenHATI is, you know, is currently based\n",
        "[00:07:11.800 --> 00:07:13.680]   on the LAMA 7 billion model.\n",
        "[00:07:13.680 --> 00:07:16.760]   But we'll be releasing many more models in different languages,\n",
        "[00:07:16.760 --> 00:07:21.960]   different sizes, and things like that as part of this series.\n",
        "[00:07:21.960 --> 00:07:24.960]   And of course, you know, we will be building further models\n",
        "[00:07:24.960 --> 00:07:27.920]   on those and doing other things to actually --\n",
        "[00:07:27.920 --> 00:07:30.040]   and we'll also have endpoints that people can use.\n",
        "[00:07:30.040 --> 00:07:32.120]   So therefore, it's not -- it's definitely, you know,\n",
        "[00:07:32.120 --> 00:07:35.340]   something that people can use to things.\n",
        "[00:07:35.340 --> 00:07:40.160]   And that's the essence of what this OpenHATI is.\n",
        "[00:07:40.160 --> 00:07:41.800]   >> So what does it mean to people in the audience here\n",
        "[00:07:41.800 --> 00:07:44.600]   who are either doing their own start-ups or a business\n",
        "[00:07:44.600 --> 00:07:49.160]   or developers, how should they look at OpenAI?\n",
        "[00:07:49.160 --> 00:07:51.540]   Sorry, Sarban, not OpenAI.\n",
        "[00:07:51.540 --> 00:07:55.720]   >> No, yeah, no, no, I think the way you look at it is\n",
        "[00:07:55.720 --> 00:07:57.640]   that we -- one of the important things\n",
        "[00:07:57.640 --> 00:08:00.640]   that we are doing is we're not just building models.\n",
        "[00:08:00.640 --> 00:08:03.920]   We are also going to be building a platform,\n",
        "[00:08:04.260 --> 00:08:09.080]   a platform for developers, where you can actually use a combination\n",
        "[00:08:09.080 --> 00:08:12.040]   of various different kinds of models, some which are from us,\n",
        "[00:08:12.040 --> 00:08:14.560]   some which are open source, some which may not be open source,\n",
        "[00:08:14.560 --> 00:08:17.720]   and actually to actually pull together and figure out how\n",
        "[00:08:17.720 --> 00:08:23.120]   to deploy, you know, generative AI applications at scale\n",
        "[00:08:23.120 --> 00:08:25.440]   and understand and evaluate their performance\n",
        "[00:08:25.440 --> 00:08:26.720]   in an efficient manner.\n",
        "[00:08:26.720 --> 00:08:29.120]   And that's something that we are planning to do at this.\n",
        "[00:08:29.120 --> 00:08:32.760]   And this platform is, you know, in the next couple of months,\n",
        "[00:08:32.760 --> 00:08:34.040]   will be coming out there.\n",
        "[00:08:34.040 --> 00:08:35.440]   It will be available to developers.\n",
        "[00:08:35.440 --> 00:08:37.200]   But, of course, those who want to start\n",
        "[00:08:37.200 --> 00:08:40.360]   with the open source things and hack for that, of course,\n",
        "[00:08:40.360 --> 00:08:42.700]   please go ahead and do that as well.\n",
        "[00:08:42.700 --> 00:08:43.520]   >> That is phenomenal.\n",
        "[00:08:43.520 --> 00:08:48.320]   But how does it compare to OpenAI itself or Google?\n",
        "[00:08:48.320 --> 00:08:53.180]   >> See, at least the things that we are doing now, right?\n",
        "[00:08:53.180 --> 00:08:55.060]   I mean, one of the things that when we thought\n",
        "[00:08:55.060 --> 00:08:58.040]   about building Sarban, we said we want\n",
        "[00:08:58.040 --> 00:09:01.520]   to build a full-stack generative AI company.\n",
        "[00:09:01.520 --> 00:09:04.760]   And different people have an understanding of a stack is\n",
        "[00:09:04.760 --> 00:09:08.240]   that we need to know how to train models from scratch.\n",
        "[00:09:08.240 --> 00:09:12.360]   We need to know how to kind of figure out how to deploy models\n",
        "[00:09:12.360 --> 00:09:13.840]   to solve real-world use cases.\n",
        "[00:09:13.840 --> 00:09:17.240]   And we need to play in the ecosystem to make sure\n",
        "[00:09:17.240 --> 00:09:22.440]   that we can actually deploy population scale applications.\n",
        "[00:09:22.440 --> 00:09:24.900]   Right? So we were thinking about all of these things.\n",
        "[00:09:24.900 --> 00:09:28.080]   But still the models we were talking about are, you know,\n",
        "[00:09:28.080 --> 00:09:29.420]   fairly small models.\n",
        "[00:09:29.420 --> 00:09:32.300]   They are fairly small models, right, the seven to maybe\n",
        "[00:09:32.300 --> 00:09:34.840]   up to 70 billion kind of range we're talking about.\n",
        "[00:09:34.840 --> 00:09:36.740]   Well, these models like OpenAI\n",
        "[00:09:36.740 --> 00:09:39.920]   and Google are obviously much bigger models, right?\n",
        "[00:09:39.920 --> 00:09:44.640]   But we want to understand the techniques and be able to build\n",
        "[00:09:44.640 --> 00:09:49.600]   that muscle to do all of these things to make it available\n",
        "[00:09:49.600 --> 00:09:49.920]   to people.\n",
        "[00:09:49.920 --> 00:09:53.040]   Now, those models are, I mean, as I said, you know,\n",
        "[00:09:53.040 --> 00:09:56.180]   I think that there is space for all of those things.\n",
        "[00:09:56.180 --> 00:09:58.640]   And I think as even Sridhar was talking\n",
        "[00:09:58.640 --> 00:10:00.640]   about earlier in the day,\n",
        "[00:10:00.640 --> 00:10:05.280]   we believe that these smaller models can do very,\n",
        "[00:10:05.280 --> 00:10:07.160]   I mean, many, many kind\n",
        "[00:10:07.160 --> 00:10:10.000]   of domain-specific tasks extremely well,\n",
        "[00:10:10.000 --> 00:10:12.360]   probably even better than the larger models.\n",
        "[00:10:12.360 --> 00:10:14.600]   And that is really one of the key areas.\n",
        "[00:10:14.600 --> 00:10:17.760]   And so therefore the value of these kinds of things, right,\n",
        "[00:10:17.760 --> 00:10:22.240]   we're not aiming in these set of models to build any AGI, right?\n",
        "[00:10:22.240 --> 00:10:23.420]   That's not our goal here.\n",
        "[00:10:23.420 --> 00:10:26.240]   Our goal is to make things that work extremely well\n",
        "[00:10:26.580 --> 00:10:30.920]   for domain-specific use cases or increase accessibility\n",
        "[00:10:30.920 --> 00:10:33.180]   through language and all of those kinds of things.\n",
        "[00:10:33.180 --> 00:10:34.940]   >> And obviously all of this unique to India.\n",
        "[00:10:34.940 --> 00:10:36.400]   But what is unique about India?\n",
        "[00:10:36.400 --> 00:10:39.320]   I mean, like, what is, is there anything special\n",
        "[00:10:39.320 --> 00:10:43.760]   in our ecosystem that makes small models focused\n",
        "[00:10:43.760 --> 00:10:46.980]   with Indian languages better, more suited for our problems?\n",
        "[00:10:46.980 --> 00:10:51.640]   >> So I think that, I mean, there are quite a few things\n",
        "[00:10:51.640 --> 00:10:53.200]   that are unique about India, right?\n",
        "[00:10:53.620 --> 00:10:57.720]   The first thing is I think that we are a voice first nation.\n",
        "[00:10:57.720 --> 00:11:01.440]   So therefore I think voice has to be the core to doing things.\n",
        "[00:11:01.440 --> 00:11:05.960]   The other thing, of course, India is extremely,\n",
        "[00:11:05.960 --> 00:11:09.660]   it's a cost-conscious country from a cost perspective.\n",
        "[00:11:09.660 --> 00:11:13.180]   Now, I would say that there are lots of interesting use cases\n",
        "[00:11:13.180 --> 00:11:16.500]   where you can use open AI and the cost structure works\n",
        "[00:11:16.500 --> 00:11:18.720]   that when, depending on your application.\n",
        "[00:11:18.720 --> 00:11:21.500]   But when you want to scale things to a massive level\n",
        "[00:11:21.740 --> 00:11:23.800]   and make it work, then you have to figure\n",
        "[00:11:23.800 --> 00:11:25.100]   out how small models work.\n",
        "[00:11:25.100 --> 00:11:28.300]   So that's something that is also specific to India.\n",
        "[00:11:28.300 --> 00:11:29.760]   The third thing which is specific\n",
        "[00:11:29.760 --> 00:11:32.780]   to India is really the success that India has had\n",
        "[00:11:32.780 --> 00:11:35.740]   in building all this digital public infrastructure.\n",
        "[00:11:35.740 --> 00:11:38.380]   When you add the AI layer on top of it,\n",
        "[00:11:38.380 --> 00:11:42.840]   then you can actually get dramatic, you know, dramatic,\n",
        "[00:11:42.840 --> 00:11:46.740]   I think, multiplicative combinatorial effects based\n",
        "[00:11:46.740 --> 00:11:47.860]   on doing things like that.\n",
        "[00:11:47.860 --> 00:11:48.840]   >> That's a phenomenal point.\n",
        "[00:11:48.840 --> 00:11:51.200]   Like, you know, it's like DPI to the power of AI almost\n",
        "[00:11:51.200 --> 00:11:54.320]   in some ways, and as a part of other, building other,\n",
        "[00:11:54.320 --> 00:11:55.820]   no better person than you.\n",
        "[00:11:55.820 --> 00:12:01.140]   So in summary, what I'm hearing is small models specialize\n",
        "[00:12:01.140 --> 00:12:04.580]   with trained with index-specific language data suited\n",
        "[00:12:04.580 --> 00:12:08.280]   for Indian problems at a compelling cost point will be\n",
        "[00:12:08.280 --> 00:12:08.860]   suited for us.\n",
        "[00:12:08.860 --> 00:12:11.480]   We're not solving some world autonomous vehicles\n",
        "[00:12:11.480 --> 00:12:12.720]   or some complex problem.\n",
        "[00:12:12.720 --> 00:12:15.780]   We're solving some basic problems specifically focused\n",
        "[00:12:15.780 --> 00:12:17.700]   on voice with multiple languages.\n",
        "[00:12:17.700 --> 00:12:19.380]   That is what you see as the future.\n",
        "[00:12:19.380 --> 00:12:21.040]   Am I paraphrasing this correctly?\n",
        "[00:12:21.340 --> 00:12:23.740]   >> No, yeah, so I think that certainly, I mean,\n",
        "[00:12:23.740 --> 00:12:27.400]   voice in Indian languages are an important part of our strategy,\n",
        "[00:12:27.400 --> 00:12:30.280]   but we will be building, you know, custom models\n",
        "[00:12:30.280 --> 00:12:33.180]   to solve various other kinds of problems as well, right?\n",
        "[00:12:33.180 --> 00:12:37.200]   It's not just limited to, I think, in different domains,\n",
        "[00:12:37.200 --> 00:12:40.180]   working different domains, making building things based\n",
        "[00:12:40.180 --> 00:12:43.440]   on unique data that enterprises have and things like that.\n",
        "[00:12:43.440 --> 00:12:44.920]   So that's something that we'll also look at.\n",
        "[00:12:44.920 --> 00:12:46.180]   >> Fair enough.\n",
        "[00:12:46.180 --> 00:12:48.900]   So coming back to the elephant in the room,\n",
        "[00:12:48.900 --> 00:12:52.780]   no pun intended with open hathi, what about Babi Shacharwal\n",
        "[00:12:52.780 --> 00:12:55.360]   and Kritarim, what is your take on that?\n",
        "[00:12:55.360 --> 00:12:56.340]   >> I think it's great.\n",
        "[00:12:56.340 --> 00:12:58.180]   I think it's wonderful, right?\n",
        "[00:12:58.180 --> 00:13:02.540]   I mean, the fact that the technology AI is so important\n",
        "[00:13:02.540 --> 00:13:05.500]   that we need multiple people working on it.\n",
        "[00:13:05.500 --> 00:13:08.380]   The fact that there are other people thinking is actually\n",
        "[00:13:08.380 --> 00:13:11.520]   validates that this is an important problem to be solved.\n",
        "[00:13:11.520 --> 00:13:16.780]   And I think that, and we need everybody to come together\n",
        "[00:13:16.780 --> 00:13:18.960]   and do that, so I really welcome that.\n",
        "[00:13:18.960 --> 00:13:20.940]   I think it's great, and I think\n",
        "[00:13:20.940 --> 00:13:24.020]   that there will be different people will have different takes\n",
        "[00:13:24.020 --> 00:13:27.420]   as to how to solve this kind of problem, and hopefully,\n",
        "[00:13:27.420 --> 00:13:30.480]   as a result of that, the entire ecosystem benefits.\n",
        "[00:13:30.480 --> 00:13:34.160]   >> One more question, and then I want to talk about some\n",
        "[00:13:34.160 --> 00:13:36.200]   of the predictions that you've boldly made.\n",
        "[00:13:36.200 --> 00:13:38.060]   So Vivek, I usually ask people\n",
        "[00:13:38.060 --> 00:13:39.200]   about what do you think the future will be,\n",
        "[00:13:39.200 --> 00:13:40.520]   and everybody usually hedges.\n",
        "[00:13:40.520 --> 00:13:42.700]   I ask Vivek, what do you think is going to happen\n",
        "[00:13:42.700 --> 00:13:44.740]   by December 2024?\n",
        "[00:13:44.740 --> 00:13:47.120]   What do you think, sitting in this room, one year later,\n",
        "[00:13:47.120 --> 00:13:49.900]   we can expect, and he made three bold predictions.\n",
        "[00:13:49.900 --> 00:13:51.560]   So I want to talk about that.\n",
        "[00:13:51.560 --> 00:13:52.980]   Before that, I have one last question.\n",
        "[00:13:52.980 --> 00:13:55.260]   What are the top three applications\n",
        "[00:13:55.260 --> 00:13:57.240]   that you think are relevant for India?\n",
        "[00:13:57.240 --> 00:13:59.580]   You heard Sridhar talk about medical.\n",
        "[00:13:59.580 --> 00:14:03.540]   Quick summary, what do you think the top three apps are\n",
        "[00:14:03.540 --> 00:14:04.380]   for India, for AI?\n",
        "[00:14:04.380 --> 00:14:09.600]   >> So, I mean, I think that, as you said, things like education\n",
        "[00:14:09.600 --> 00:14:13.400]   and medical are clearly areas where I think\n",
        "[00:14:13.400 --> 00:14:15.020]   that things can be leveraged.\n",
        "[00:14:15.020 --> 00:14:19.440]   The whole idea of all these kind of the DPI aspect\n",
        "[00:14:19.440 --> 00:14:21.100]   of it is another major application\n",
        "[00:14:21.100 --> 00:14:22.100]   where things can happen.\n",
        "[00:14:22.100 --> 00:14:24.500]   And here I'm talking about country-specific work.\n",
        "[00:14:24.500 --> 00:14:26.940]   And I think the whole idea, which Sridhar also talked\n",
        "[00:14:26.940 --> 00:14:29.940]   about was the concept of software, right?\n",
        "[00:14:29.940 --> 00:14:33.300]   And I think that, and clearly we have a very large software\n",
        "[00:14:33.300 --> 00:14:36.140]   industry, and how to reimagine those things\n",
        "[00:14:36.140 --> 00:14:38.200]   in this context is also something that's going to be.\n",
        "[00:14:38.200 --> 00:14:39.940]   >> Okay. Fair enough.\n",
        "[00:14:40.360 --> 00:14:43.740]   Are you guys ready for Vivek Raghavan's bold predictions?\n",
        "[00:14:43.740 --> 00:14:45.680]   Yes? No?\n",
        "[00:14:45.680 --> 00:14:46.720]   I'm not hearing any, yes, sir.\n",
        "[00:14:46.720 --> 00:14:47.640]   This is like a big deal.\n",
        "[00:14:47.640 --> 00:14:49.680]   He's like one of the smartest guys that I know.\n",
        "[00:14:49.680 --> 00:14:51.260]   He wants to make three predictions.\n",
        "[00:14:51.260 --> 00:14:53.420]   You don't want to hear it.\n",
        "[00:14:53.420 --> 00:14:54.220]   All right.\n",
        "[00:14:54.220 --> 00:14:57.760]   So, I asked him, what do you think, you know, a year later,\n",
        "[00:14:57.760 --> 00:14:59.060]   what do you think we can expect?\n",
        "[00:14:59.060 --> 00:15:00.900]   And he came up with three things,\n",
        "[00:15:00.900 --> 00:15:02.920]   and usually people give very blunt answers\n",
        "[00:15:02.920 --> 00:15:04.360]   when you ask a question like this,\n",
        "[00:15:04.360 --> 00:15:06.660]   because they don't want to be caught wrong, not Vivek.\n",
        "[00:15:06.660 --> 00:15:07.580]   Vivek is bold.\n",
        "[00:15:07.580 --> 00:15:10.000]   So, he basically said three things,\n",
        "[00:15:10.000 --> 00:15:11.440]   and I'm going to list out the three things,\n",
        "[00:15:11.440 --> 00:15:13.080]   and then he's asked him about it.\n",
        "[00:15:13.080 --> 00:15:16.140]   So, number one, he says, I will prefer to talk\n",
        "[00:15:16.140 --> 00:15:19.200]   to an automated customer service than a real person,\n",
        "[00:15:19.200 --> 00:15:21.300]   because they'll give me a better answer.\n",
        "[00:15:21.300 --> 00:15:23.580]   So, that is Vivek Raghavan's prediction, number one.\n",
        "[00:15:23.580 --> 00:15:27.680]   So, number two is that when everybody is talking\n",
        "[00:15:27.680 --> 00:15:30.660]   about a GPU shortage, Vivek predicts\n",
        "[00:15:30.660 --> 00:15:32.600]   that there'll be a GPU glut in India.\n",
        "[00:15:32.600 --> 00:15:34.460]   He thinks there'll be too much GPU, okay?\n",
        "[00:15:34.460 --> 00:15:38.440]   So, if you want to short in media stock, this is a good time.\n",
        "[00:15:39.060 --> 00:15:41.400]   And number three, which was extremely unexpected,\n",
        "[00:15:41.400 --> 00:15:45.320]   he said some companies will suddenly die, okay?\n",
        "[00:15:45.320 --> 00:15:47.780]   So, Vivek, these are not what I expected.\n",
        "[00:15:47.780 --> 00:15:52.840]   So, you want to quickly talk about each of them,\n",
        "[00:15:52.840 --> 00:15:54.520]   why you just came up with these,\n",
        "[00:15:54.520 --> 00:15:56.660]   and then we'll throw the open for audience questions.\n",
        "[00:15:56.660 --> 00:15:59.200]   >> So, I don't think I quite said it the way that --\n",
        "[00:15:59.200 --> 00:16:01.480]   >> Sorry, I'm a marketing guy, yes.\n",
        "[00:16:01.480 --> 00:16:03.740]   >> But it's interesting.\n",
        "[00:16:03.740 --> 00:16:09.300]   But I think the first thing that we said is I think that --\n",
        "[00:16:09.300 --> 00:16:10.500]   and I don't think that this is --\n",
        "[00:16:10.500 --> 00:16:16.100]   I think there will come a time when, you know, in areas\n",
        "[00:16:16.100 --> 00:16:17.600]   of customer service, et cetera,\n",
        "[00:16:17.600 --> 00:16:20.380]   when you want to do something very specific.\n",
        "[00:16:20.380 --> 00:16:23.840]   Today, you know, when you call some kind of a bot,\n",
        "[00:16:23.840 --> 00:16:26.860]   you actually end up -- you mostly try to disconnect the call\n",
        "[00:16:26.860 --> 00:16:29.420]   or, you know, you're extremely upset\n",
        "[00:16:29.420 --> 00:16:30.860]   that you're talking to a bot.\n",
        "[00:16:30.860 --> 00:16:32.980]   But I think that there will come a time.\n",
        "[00:16:32.980 --> 00:16:35.500]   And I'm predicting it is sooner than later\n",
        "[00:16:35.500 --> 00:16:39.440]   that you will actually get better responses from the bot\n",
        "[00:16:39.440 --> 00:16:41.920]   than what the human representative that --\n",
        "[00:16:41.920 --> 00:16:43.740]   at least the average human representative\n",
        "[00:16:43.740 --> 00:16:45.540]   that you could talk to could give.\n",
        "[00:16:45.540 --> 00:16:47.700]   And I think that that's just a --\n",
        "[00:16:47.700 --> 00:16:51.120]   I just said that there will come a time where you know it's not a\n",
        "[00:16:51.120 --> 00:16:54.580]   human you're talking to, but it's probably more likely\n",
        "[00:16:54.580 --> 00:16:57.940]   to solve your intent than the human person.\n",
        "[00:16:57.940 --> 00:17:03.100]   So that's just something that I think that could happen.\n",
        "[00:17:03.100 --> 00:17:04.820]   >> Okay. Definitely controversial.\n",
        "[00:17:04.820 --> 00:17:06.380]   But we'll let it go.\n",
        "[00:17:06.380 --> 00:17:07.620]   What about the GPU glut?\n",
        "[00:17:07.620 --> 00:17:08.940]   >> No, no, yeah.\n",
        "[00:17:08.940 --> 00:17:11.760]   So I don't think that -- so I think that the fact\n",
        "[00:17:11.760 --> 00:17:14.940]   that there is tremendous shortage right now,\n",
        "[00:17:14.940 --> 00:17:16.740]   I think that shortage will ease\n",
        "[00:17:16.740 --> 00:17:19.860]   because that is how the cycles of things go, right?\n",
        "[00:17:19.860 --> 00:17:22.680]   When, you know, I think the fact that there was\n",
        "[00:17:22.680 --> 00:17:25.440]   such a severe shortage last year, you know,\n",
        "[00:17:25.520 --> 00:17:29.300]   basically caused a number of different players to ramp\n",
        "[00:17:29.300 --> 00:17:31.080]   up in various kinds of forms.\n",
        "[00:17:31.080 --> 00:17:34.420]   And I think that that will always go in a cycle.\n",
        "[00:17:34.420 --> 00:17:37.080]   But you may -- we may find out that there are many,\n",
        "[00:17:37.080 --> 00:17:38.640]   many more interesting problems\n",
        "[00:17:38.640 --> 00:17:40.340]   that people will be able to solve.\n",
        "[00:17:40.340 --> 00:17:48.840]   I still remember, you know, we were at a Gen AI event in Bangalore\n",
        "[00:17:48.840 --> 00:17:51.360]   and we were talking to people and we said, you know,\n",
        "[00:17:51.360 --> 00:17:55.300]   how many people have access to, you know, four A100s?\n",
        "[00:17:55.300 --> 00:17:56.780]   This was the question that I'd asked.\n",
        "[00:17:56.780 --> 00:17:57.820]   And nobody in the room.\n",
        "[00:17:57.820 --> 00:18:00.240]   And these are all extremely enthusiastic Gen AI people.\n",
        "[00:18:00.240 --> 00:18:01.400]   And nobody had access.\n",
        "[00:18:01.400 --> 00:18:03.460]   And I think that thing is going to change.\n",
        "[00:18:03.460 --> 00:18:05.500]   You will be able to get these kinds of things.\n",
        "[00:18:05.500 --> 00:18:08.340]   And people who want to hack and do things will have access\n",
        "[00:18:08.340 --> 00:18:13.160]   to these things at -- without, you know, having to write a,\n",
        "[00:18:13.160 --> 00:18:15.040]   you know, a major check to do.\n",
        "[00:18:15.040 --> 00:18:17.320]   >> Vivek is also a semi-conductor guy before he went\n",
        "[00:18:17.320 --> 00:18:20.360]   into Adar, so I would take his predictions very seriously.\n",
        "[00:18:20.360 --> 00:18:22.620]   So I don't know what I -- I'm going to sell my immediate stock.\n",
        "[00:18:22.620 --> 00:18:24.560]   >> I would not do that.\n",
        "[00:18:24.560 --> 00:18:26.020]   But that's not what I said.\n",
        "[00:18:26.020 --> 00:18:30.280]   >> I want to blame you for this if it goes up.\n",
        "[00:18:30.280 --> 00:18:32.560]   But the third one is pretty strange.\n",
        "[00:18:32.560 --> 00:18:34.940]   You know, companies are born, companies die.\n",
        "[00:18:34.940 --> 00:18:37.660]   But you said some companies will suddenly die.\n",
        "[00:18:37.660 --> 00:18:38.540]   What does that mean?\n",
        "[00:18:38.540 --> 00:18:42.920]   >> No, I think -- see, I think the interesting thing is --\n",
        "[00:18:42.920 --> 00:18:46.560]   and I think that it comes back to the fundamental nature of AI.\n",
        "[00:18:47.160 --> 00:18:50.420]   AI is a tool, right, and you have to use that.\n",
        "[00:18:50.420 --> 00:18:54.440]   And you have to use that within your business process, right?\n",
        "[00:18:54.440 --> 00:18:56.440]   And how AI is being used.\n",
        "[00:18:56.440 --> 00:18:58.540]   And so -- and what's going to happen is that --\n",
        "[00:18:58.540 --> 00:19:02.740]   I mean, I think this is true with, you know, when someone said\n",
        "[00:19:02.740 --> 00:19:06.020]   in terms of, you know, people, they said that the people\n",
        "[00:19:06.020 --> 00:19:10.280]   who leverage AI will be more effective than those\n",
        "[00:19:10.280 --> 00:19:11.220]   who don't leverage AI.\n",
        "[00:19:11.220 --> 00:19:14.220]   And that will -- it's true for organizations also.\n",
        "[00:19:14.520 --> 00:19:17.160]   Organizations that leverage AI in --\n",
        "[00:19:17.160 --> 00:19:20.500]   fundamentally in their core business processes will be more\n",
        "[00:19:20.500 --> 00:19:22.760]   effective than those who don't, right?\n",
        "[00:19:22.760 --> 00:19:23.920]   And I think that's the thing.\n",
        "[00:19:23.920 --> 00:19:26.560]   And you won't know the difference\n",
        "[00:19:26.560 --> 00:19:30.180]   until one day it becomes too obvious, and it will be too late.\n",
        "[00:19:30.180 --> 00:19:33.640]   And I think that's the reason why everybody needs to think\n",
        "[00:19:33.640 --> 00:19:36.720]   about what it means for your business.\n",
        "[00:19:36.720 --> 00:19:39.200]   Because you will -- everything will be fine.\n",
        "[00:19:39.200 --> 00:19:40.380]   Everything will be fine.\n",
        "[00:19:40.380 --> 00:19:45.040]   And one day, somebody in your -- either your competitor\n",
        "[00:19:45.040 --> 00:19:47.120]   in your space or somebody brand new coming\n",
        "[00:19:47.120 --> 00:19:49.480]   into your space will be reimagining your business\n",
        "[00:19:49.480 --> 00:19:50.880]   process completely.\n",
        "[00:19:50.880 --> 00:19:53.780]   And at that stage, you will find that it's, you know,\n",
        "[00:19:53.780 --> 00:19:58.840]   it's a very big, very tall, you know, mountain to climb.\n",
        "[00:19:58.840 --> 00:20:02.220]   And that's why I think it's important for both people\n",
        "[00:20:02.220 --> 00:20:05.960]   and entities to think about how they will, you know,\n",
        "[00:20:05.960 --> 00:20:08.880]   they will upgrade themselves, or they will modify their business\n",
        "[00:20:08.880 --> 00:20:10.880]   processes to a -- you know, to a --\n",
        "[00:20:10.880 --> 00:20:12.040]   >> That's a very nuanced answer.\n",
        "[00:20:12.040 --> 00:20:14.840]   And everybody here who's running a business should really think\n",
        "[00:20:14.840 --> 00:20:17.900]   about it, because life will be the same, and then suddenly,\n",
        "[00:20:17.900 --> 00:20:19.280]   suddenly something will, you know,\n",
        "[00:20:19.280 --> 00:20:20.620]   then there will be a step change.\n",
        "[00:20:20.620 --> 00:20:22.840]   We make -- I have a few more questions,\n",
        "[00:20:22.840 --> 00:20:25.180]   but I'm sure the audience has a lot of questions for you.\n",
        "[00:20:25.180 --> 00:20:27.020]   So how are we doing on time?\n",
        "[00:20:27.020 --> 00:20:31.040]   Okay. So that's -- okay.\n",
        "[00:20:31.040 --> 00:20:32.640]   A lot of questions, so I'd love to --\n",
        "[00:20:32.640 --> 00:20:35.000]   is there a mic that we can pass it on?\n",
        "[00:20:35.000 --> 00:20:40.120]   [ Pause ]\n",
        "[00:20:40.120 --> 00:20:40.920]   >> Thank you.\n",
        "[00:20:40.920 --> 00:20:42.040]   My name is Karthik.\n",
        "[00:20:42.040 --> 00:20:45.960]   I work for IT service industry.\n",
        "[00:20:45.960 --> 00:20:52.200]   So you're saying that you're working on LLM -- sorry,\n",
        "[00:20:52.200 --> 00:20:54.700]   it's fine-tuned LLM on top of LAMA.\n",
        "[00:20:54.700 --> 00:20:57.000]   My basic question, fundamental question is,\n",
        "[00:20:57.000 --> 00:20:59.560]   we don't have a foundational model for India.\n",
        "[00:20:59.560 --> 00:21:03.240]   Most of the models are basically using English\n",
        "[00:21:03.240 --> 00:21:04.840]   or those kind of things.\n",
        "[00:21:04.840 --> 00:21:06.880]   For example, you know, Andrew was talking\n",
        "[00:21:06.880 --> 00:21:10.080]   about the tokenizers and things like that.\n",
        "[00:21:10.080 --> 00:21:12.160]   So are you working on anything like that?\n",
        "[00:21:12.160 --> 00:21:16.040]   Or do you want to use mostly the existing models\n",
        "[00:21:16.040 --> 00:21:16.940]   and run on top of it?\n",
        "[00:21:16.940 --> 00:21:18.720]   >> You asked a good question.\n",
        "[00:21:18.720 --> 00:21:20.080]   You asked a cherry question for himself.\n",
        "[00:21:20.080 --> 00:21:24.700]   >> No, I think the interesting thing is that if you look at --\n",
        "[00:21:24.700 --> 00:21:28.080]   and then we have actually a blog on this, on our website.\n",
        "[00:21:28.080 --> 00:21:30.200]   I think one of the things that we've actually built,\n",
        "[00:21:30.200 --> 00:21:32.520]   a customized tokenizer,\n",
        "[00:21:32.520 --> 00:21:36.080]   which actually fundamentally changes the cost of some\n",
        "[00:21:36.080 --> 00:21:38.160]   of these generations in Indian languages.\n",
        "[00:21:38.160 --> 00:21:41.440]   And I think that we're not just fine-tuning.\n",
        "[00:21:41.440 --> 00:21:44.440]   We're actually -- we are leveraging the existing\n",
        "[00:21:44.440 --> 00:21:46.200]   pre-training, but we are doing what's known\n",
        "[00:21:46.200 --> 00:21:48.020]   as continual pre-training, which actually --\n",
        "[00:21:48.020 --> 00:21:52.760]   but having said that, you know, I think that once we have\n",
        "[00:21:52.760 --> 00:21:54.240]   to figure out where is the data\n",
        "[00:21:54.240 --> 00:21:56.500]   to train an extremely large model from scratch.\n",
        "[00:21:56.500 --> 00:21:57.800]   And some of those things are things\n",
        "[00:21:57.800 --> 00:22:00.380]   which will happen over time.\n",
        "[00:22:00.520 --> 00:22:04.360]   But I think that, yes, I think\n",
        "[00:22:04.360 --> 00:22:06.640]   that we will be doing various kinds of things.\n",
        "[00:22:06.640 --> 00:22:09.640]   But the interesting thing is that if I want\n",
        "[00:22:09.640 --> 00:22:11.520]   to change the accessibility problem\n",
        "[00:22:11.520 --> 00:22:14.380]   with an existing open source model, how do I do that?\n",
        "[00:22:14.380 --> 00:22:15.900]   And that's the problem that we have --\n",
        "[00:22:15.900 --> 00:22:19.180]   that we think we have solved, and it's going to be the heart\n",
        "[00:22:19.180 --> 00:22:20.680]   of this open RT series.\n",
        "[00:22:20.680 --> 00:22:22.040]   >> Extremely well explained in the blog.\n",
        "[00:22:22.040 --> 00:22:23.440]   Even I could understand it, so.\n",
        "[00:22:23.440 --> 00:22:26.440]   >> Hi. I'm Prishant.\n",
        "[00:22:26.440 --> 00:22:28.060]   I work for a fintech company.\n",
        "[00:22:28.240 --> 00:22:30.720]   My question is, like, unlike China,\n",
        "[00:22:30.720 --> 00:22:34.300]   we never had a consumer-facing application coming\n",
        "[00:22:34.300 --> 00:22:38.620]   out from India, and in Web1, Web2, crypto and all.\n",
        "[00:22:38.620 --> 00:22:41.780]   Why do you think it will be different this time in, like,\n",
        "[00:22:41.780 --> 00:22:50.640]   AI, because will the DPI and other things will serve the same\n",
        "[00:22:50.640 --> 00:22:53.840]   purpose what the great firewall did in China, or do you think,\n",
        "[00:22:53.840 --> 00:22:57.400]   like, because AI is a strategic sector,\n",
        "[00:22:57.960 --> 00:23:02.900]   no outside country can work in NASA projects.\n",
        "[00:23:02.900 --> 00:23:05.400]   Maybe all government contact will go to them.\n",
        "[00:23:05.400 --> 00:23:08.560]   What is at least the moat here for an Indian company?\n",
        "[00:23:08.560 --> 00:23:15.640]   >> So I don't -- I think the question is --\n",
        "[00:23:15.640 --> 00:23:18.580]   I don't know the answer to these questions, right?\n",
        "[00:23:18.580 --> 00:23:22.240]   I mean, I think that it's difficult to predict.\n",
        "[00:23:22.240 --> 00:23:25.000]   But I do believe, and as I'm repeating,\n",
        "[00:23:25.280 --> 00:23:28.880]   that the combinatorial effect of being using Gen-AI\n",
        "[00:23:28.880 --> 00:23:32.520]   at a large scale, in addition, along with the DPI work\n",
        "[00:23:32.520 --> 00:23:34.800]   that we've done in India, will have people.\n",
        "[00:23:34.800 --> 00:23:37.400]   And I think that, in the end, it is --\n",
        "[00:23:37.400 --> 00:23:41.280]   the intent is that people need to be able to use it,\n",
        "[00:23:41.280 --> 00:23:43.720]   and they will vote by things that are useful for them.\n",
        "[00:23:43.720 --> 00:23:46.240]   And if that doesn't happen, you're right.\n",
        "[00:23:46.240 --> 00:23:49.580]   That -- and I think that we have to figure\n",
        "[00:23:49.580 --> 00:23:52.600]   out what is the mechanism of delivery of apps, right?\n",
        "[00:23:52.600 --> 00:23:56.400]   I mean, how -- where do Indians consume content?\n",
        "[00:23:56.400 --> 00:23:57.400]   That's the question.\n",
        "[00:23:57.400 --> 00:23:59.200]   >> Yeah, I'm so sorry, but we are out of time.\n",
        "[00:23:59.200 --> 00:24:00.480]   Vivek will be outside.\n",
        "[00:24:00.480 --> 00:24:00.920]   >> Yeah.\n",
        "[00:24:00.920 --> 00:24:03.200]   >> So he would be able to answer the question.\n",
        "[00:24:03.200 --> 00:24:04.440]   Do we have time for one last question?\n",
        "[00:24:04.440 --> 00:24:06.200]   >> Can I just take one last?\n",
        "[00:24:06.200 --> 00:24:07.560]   Yeah. Thank you.\n",
        "[00:24:07.560 --> 00:24:07.920]   Thank you.\n",
        "[00:24:07.920 --> 00:24:08.960]   I'm Manish Kuthari.\n",
        "[00:24:08.960 --> 00:24:10.680]   I'm from ISBR Business School.\n",
        "[00:24:10.680 --> 00:24:12.920]   Good that I got a chance to ask you this question.\n",
        "[00:24:12.920 --> 00:24:16.120]   During lunchtime, there were a few of our educationists\n",
        "[00:24:16.120 --> 00:24:18.280]   whom we were talking about, and we were --\n",
        "[00:24:18.280 --> 00:24:21.460]   there was one from school, and we are from the MBA institutions.\n",
        "[00:24:21.460 --> 00:24:23.920]   We were thinking of these present generations.\n",
        "[00:24:23.920 --> 00:24:26.780]   How do we get them into what you are doing?\n",
        "[00:24:26.780 --> 00:24:29.520]   There is one thing that they have been regularly\n",
        "[00:24:29.520 --> 00:24:31.560]   that the concentrations that they are working on,\n",
        "[00:24:31.560 --> 00:24:34.240]   but artificial intelligence and getting into this,\n",
        "[00:24:34.240 --> 00:24:36.640]   getting them into their academics and making them a part\n",
        "[00:24:36.640 --> 00:24:39.200]   of it is very important, including the trainers\n",
        "[00:24:39.200 --> 00:24:41.680]   who train them, making them future-ready\n",
        "[00:24:41.680 --> 00:24:43.680]   into what you are doing is amazing.\n",
        "[00:24:43.680 --> 00:24:47.680]   And the speed that -- which is growing, it is calling for a lot\n",
        "[00:24:47.680 --> 00:24:49.640]   of training that needs to be done.\n",
        "[00:24:50.280 --> 00:24:52.280]   Can you, from your angle, throw some light\n",
        "[00:24:52.280 --> 00:24:55.000]   on how we could make them future-ready?\n",
        "[00:24:55.000 --> 00:24:58.480]   How these people who are management graduates\n",
        "[00:24:58.480 --> 00:25:01.980]   and from schools who are coming out, how do we get into this part\n",
        "[00:25:01.980 --> 00:25:03.560]   of technology that you spoke about?\n",
        "[00:25:03.560 --> 00:25:07.240]   Oh, so this is really a challenge,\n",
        "[00:25:07.240 --> 00:25:10.360]   because I think everyone will need to understand\n",
        "[00:25:10.360 --> 00:25:13.120]   at some level what this technology does.\n",
        "[00:25:13.120 --> 00:25:15.440]   And I think that we have to rethink\n",
        "[00:25:15.440 --> 00:25:18.160]   how we get everyone into these --\n",
        "[00:25:18.160 --> 00:25:22.160]   and this kind of education has to be at many different levels.\n",
        "[00:25:22.160 --> 00:25:25.080]   There are, from a core set of having people\n",
        "[00:25:25.080 --> 00:25:28.520]   who are extremely good at some --\n",
        "[00:25:28.520 --> 00:25:30.080]   and there you don't need as many,\n",
        "[00:25:30.080 --> 00:25:32.600]   but then there are basically vast numbers of people\n",
        "[00:25:32.600 --> 00:25:34.320]   who can actually leverage these tools.\n",
        "[00:25:34.320 --> 00:25:36.800]   By the way, the most important thing about --\n",
        "[00:25:36.800 --> 00:25:39.580]   and maybe that's part of what makes an LLM interesting --\n",
        "[00:25:39.580 --> 00:25:44.560]   is that how you use it, your mileage varies by that.\n",
        "[00:25:44.560 --> 00:25:47.440]   And to understand how to actually leverage this\n",
        "[00:25:47.440 --> 00:25:49.000]   in an interesting way is something\n",
        "[00:25:49.000 --> 00:25:52.600]   that we have to widely teach many, many people.\n",
        "[00:25:52.600 --> 00:25:57.520]   And because asking the things in the right way\n",
        "[00:25:57.520 --> 00:25:59.320]   and having the right kind of applications\n",
        "[00:25:59.320 --> 00:26:03.100]   will make a huge difference to how people can leverage these tools.\n",
        "[00:26:03.100 --> 00:26:04.240]   Awesome. Thank you.\n",
        "[00:26:04.240 --> 00:26:05.760]   Thank you very much, Vivek.\n",
        "[00:26:05.760 --> 00:26:07.600]   Very good luck to Sarvam and good luck to India.\n",
        "[00:26:07.600 --> 00:26:10.880]   I think it's going to be a lot right on your shoulders.\n",
        "[00:26:10.880 --> 00:26:13.720]   Thanks, Bala.\n",
        "[00:26:13.720 --> 00:26:14.880]   Thank you, Mr. Raghavan.\n",
        "\"\"\"\n",
        "\n",
        "timestamps, transcripts = parse_output(output)\n",
        "alignment, alignment_path = align_transcription_with_audio(\"/content/Sarvam_output16000.wav\", timestamps, transcripts)\n",
        "\n",
        "# Print the alignment and alignment path\n",
        "print(\"Alignment:\")\n",
        "for segment in alignment:\n",
        "    print(segment)\n",
        "\n",
        "print(\"Alignment Path:\")\n",
        "for path in alignment_path:\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5M0qFRmfKPN"
      },
      "source": [
        "Testing if correctly aligned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9usny5-Qd_as",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "e35b0248-01d7-4859-d8d4-bfb972a789ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pydub'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5c98c7c53649>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def play_audio_segment(audio_file, start_time, end_time):\n",
        "    audio = AudioSegment.from_wav(audio_file)\n",
        "    segment = audio[start_time * 1000:end_time * 1000]  # Convert seconds to milliseconds\n",
        "\n",
        "    # Create a temporary file to save the segment\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_file:\n",
        "        segment.export(temp_file.name, format=\"wav\")\n",
        "        temp_file.close()\n",
        "        display(Audio(temp_file.name))\n",
        "        os.remove(temp_file.name)\n",
        "\n",
        "# Example usage:\n",
        "for start_time, end_time, text in alignment:\n",
        "    print(f\"Playing segment: {text}\")\n",
        "    play_audio_segment(\"/content/Sarvam_output16000.wav\", start_time, end_time)\n",
        "    time.sleep(5)  # Wait for 5 seconds before playing the next segment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5db0Zo6Dfa4l"
      },
      "source": [
        "Semantic chunking (each chunk/segment length < 15 secs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5231jxjeN8V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7NS7T_1eiu_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "# Assuming we have transcript alignments in the format: (start_time, end_time, text)\n",
        "\n",
        "# Load audio file\n",
        "audio_file = \"/content/Sarvam_output16000.wav\"\n",
        "audio, sr = librosa.load(audio_file, sr=None)\n",
        "\n",
        "# Split audio into chunks of approximately 15 seconds each\n",
        "chunk_duration = 15  # in seconds\n",
        "num_chunks = int(np.ceil(len(audio) / sr / chunk_duration))\n",
        "chunk_size = int(len(audio) / num_chunks)\n",
        "\n",
        "# Create output directories\n",
        "output_audio_dir = \"/content/audio_chunks\"\n",
        "output_transcript_file = \"/content/transcript_chunks.txt\"\n",
        "os.makedirs(output_audio_dir, exist_ok=True)\n",
        "\n",
        "# Semantic chunking\n",
        "semantic_chunks = []\n",
        "final_output_list = []\n",
        "for i in range(num_chunks):\n",
        "    start_index = i * chunk_size\n",
        "    end_index = min((i + 1) * chunk_size, len(audio))\n",
        "    audio_chunk = audio[start_index:end_index]\n",
        "    start_time = start_index / sr\n",
        "    end_time = end_index / sr\n",
        "\n",
        "    # Find transcript alignments corresponding to the audio chunk\n",
        "    corresponding_alignments = []\n",
        "    for item in alignment:\n",
        "        alignment_start, alignment_end, alignment_text = item\n",
        "        if start_index / sr <= alignment_start < end_index / sr or start_index / sr < alignment_end <= end_index / sr:\n",
        "            corresponding_alignments.append(item)\n",
        "\n",
        "    if corresponding_alignments:\n",
        "        semantic_chunks.append((audio_chunk, corresponding_alignments))\n",
        "\n",
        "       # Save audio chunk\n",
        "        audio_chunk_filename = os.path.join(output_audio_dir, f\"chunk_{i + 1}.wav\")\n",
        "        sf.write(audio_chunk_filename, audio_chunk, sr)\n",
        "\n",
        "        # Create final output list\n",
        "        combined_text = \" \".join([alignment_text for _, _, alignment_text in corresponding_alignments])\n",
        "        final_output_list.append({\n",
        "            \"chunk_id\": i + 1,\n",
        "            \"chunk_length\": len(audio_chunk) / sr,\n",
        "            \"text\": combined_text,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "        })\n",
        "\n",
        "# Save transcript alignments to a text file\n",
        "with open(output_transcript_file, \"w\") as f:\n",
        "    for item in final_output_list:\n",
        "        f.write(f\"Segment {item['chunk_id']}:\\n\")\n",
        "        f.write(f\"Audio Filename: chunk_{item['chunk_id']}.wav\\n\")\n",
        "        f.write(f\"Audio Duration: {item['chunk_length']} seconds\\n\")\n",
        "        f.write(f\"Start Time: {item['start_time']}\\n\")\n",
        "        f.write(f\"End Time: {item['end_time']}\\n\")\n",
        "        f.write(f\"Transcript: {item['text']}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(final_output_list)\n",
        "print(\"Audio chunks and transcripts have been saved.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}